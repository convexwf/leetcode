
## 设计模式

单例模式
意图：保证一个类仅有一个实例，并提供一个访问它的全局访问点。
关键代码：构造函数是私有的。
这种模式涉及到一个单一的类，该类负责创建自己的对象，同时确保只有单个对象被创建。这个类提供了一种访问其唯一的对象的方式，可以直接访问，不需要实例化该类的对象。
使用场景： 1、要求生产唯一序列号。2. 创建的一个对象需要消耗的资源过多，比如 I/O 与数据库的连接等。连接池。
实现方式
懒汉式: 调用时候才实例化，线程不安全
饿汉式：类加载时就初始化，浪费内存。

工厂模式
在工厂模式中，我们在创建对象时不会对客户端暴露创建逻辑，并且是通过使用一个共同的接口来指向新创建的对象。
意图：定义一个创建对象的接口，让其子类自己决定实例化哪一个工厂类，工厂模式使其创建过程延迟到子类进行。
主要解决：主要解决接口选择的问题。
何时使用：我们明确地计划不同条件下创建不同实例时。
如何解决：让其子类实现工厂接口，返回的也是一个抽象的产品。
关键代码：创建过程在其子类执行。

```go
创建一个接口
// 创建一个接口
type Shape interface {
    Draw()
}

// 实现接口的实体类
type Rectangle struct {
}

func (this Rectangle) Draw() {
    fmt.Println("Inside Rectangle::draw() method.")
}

type Square struct {
}

func (this Square) Draw() {
    fmt.Println("Inside Square ::draw() method.")
}

type Circle struct {
}

func (this Circle) Draw() {
    fmt.Println("Inside Circle  ::draw() method.")
}

// 工厂
type ShapeFactory struct {
}

//使用 getShape 方法获取形状类型的对象
func (this ShapeFactory) getShape(shapeType string) Shape {
    if shapeType == "" {
        return nil
    }
    if shapeType == "CIRCLE" {
        return Circle{}
    } else if shapeType == "RECTANGLE" {
        return Rectangle{}
    } else if shapeType == "SQUARE" {
        return Square{}
    }
    return nil
}

// 使用该工厂，通过传递类型信息来获取实体类的对象。
func main() {
    factory := ShapeFactory{}
    factory.getShape("CIRCLE").Draw()
    factory.getShape("RECTANGLE").Draw()
    factory.getShape("SQUARE").Draw()
}

```

观察者模式
当对象间存在一对多关系时，则使用观察者模式（Observer Pattern）。比如，当一个对象被修改时，则会自动通知它的依赖对象。观察者模式属于行为型模式。
意图：定义对象间的一种一对多的依赖关系，当一个对象的状态发生改变时，所有依赖于它的对象都得到通知并被自动更新。
主要解决：一个对象状态改变给其他对象通知的问题，而且要考虑到易用和低耦合，保证高度的协作。
何时使用：一个对象（目标对象）的状态发生改变，所有的依赖对象（观察者对象）都将得到通知，进行广播通知。
如何解决：使用面向对象技术，可以将这种依赖关系弱化。
关键代码：在抽象类里有一个 ArrayList 存放观察者们。

装饰器模式
动态的为一个对象增加功能，而且还能动态撤销。不使用继承（继承不能做到这一点，继承的功能是静态的，不能动态增删。）
装饰器模式（Decorator Pattern）允许向一个现有的对象添加新的功能，同时又不改变其结构。这种类型的设计模式属于结构型模式，它是作为现有的类的一个包装。
这种模式创建了一个装饰类，用来包装原有的类，并在保持类方法签名完整性的前提下，提供了额外的功能。
我们通过下面的实例来演示装饰器模式的用法。其中，我们将把一个形状装饰上不同的颜色，同时又不改变形状类。

## C++

const只在编译期有用，在运行期无用
sizeof是运算符，在编译时即计算好了； 而strlen是函数，要在运行时才能计算。


const int *a: 指向常整型数的指针
int * const a: 指向整型数的常指针
int const *const a 



虚函数如何实现多态
答：基类的指针指向子类的对象，子类对象重写了基类中的虚函数，那么在用基类指针操作该函数时就会根据其指向对象的类型（基类还是子类）分别调用不同的函数。
这是通过虚函数表实现的。只要类定义中含有虚函数声明，那么编译器就会为该类建立对应的虚函数表，按照虚函数声明顺序将函数入口地址保存在虚函数表中，这个表不占用对象内存，但是编译器会为对象分配一个指向虚函数表地址的指针vptr，在对象中占有一个指针大小的内存。子类重写的虚函数的地址直接替换了父类虚函数在虚函数表中的位置，而子类中独有的虚函数在其虚函数表中会依次排在从父类继承的虚函数后面。
在实际调用函数时，编译器先检查该函数是否为虚函数，如果是，则在对象的虚函数表中找到函数入口地址进行调用。对于在子类总重写的虚函数，这样就实现了多态。

Q: 动态绑定
A: C++中，通过基类的引用或指针调用虚函数时，发生动态绑定。引用（或指针）既可以指向基类对象也可以指向派生类对象，这一事实是动态绑定的关键。用引用（或指针）调用的虚函数在运行时确定，被调用的函数是引用（或指针）所指对象的实际类型所定义的。
C++中动态绑定是通过虚函数实现的。而虚函数是通过一张虚函数表（virtualtable）实现的。这个表中记录了虚函数的地址，解决继承、覆盖的问题，保证动态绑定时能够根据对象的实际类型调用正确的函数。
把一个方法与其所在的类/对象关联起来叫做方法的绑定。绑定分为静态绑定（前期绑定）和动态绑定（后期绑定）。静态绑定（前期绑定）是指在程序运行前就已经知道方法是属于那个类的，在编译的时候就可以连接到类的中，定位到这个方法。动态绑定（后期绑定）是指在程序运行过程中，根据具体的实例对象才能具体确定是哪个方法。
静态绑定发生于数据结构和数据结构间，程序执行之前。静态绑定发生于编译期，因此不能利用任何运行期的信息。它针对函数调用与函数的主体，或变量与内存中的区块。动态绑定则针对运行期产生的访问请求，只用到运行期的可用信息。在面向对象的代码中，动态绑定意味着决定哪个方法被调用或哪个属性被访问，将基于这个类本身而不基于访问范围。

Q: volatile 关键字
A: volatile提醒编译器它后面所定义的变量随时都有可能改变，因此编译后的程序每次需要存储或读取这个变量的时候，都会直接从变量地址中读取数据。如果没有volatile关键字，则编译器可能优化读取和存储，可能暂时使用寄存器中的值，如果这个变量由别的程序更新了的话，将出现不一致的现象。


Q: 栈溢出
A:
![](res/2021-03-29-21-16-26.png)

Q: 什么时候会发生段错误
A:
段错误常发生在访问非法内存地址的时候
使用野指针
视图修改字符串常量的内容

Q: C++11特性
A:
auto关键字
nullptr
智能指针
初始化列表
右值引用、移动语义、完美转发
新增stl容器array和tuple

Q: 可变参数模板
A:
可变参数模板对参数进行了高度泛化，可以表示任意数目、任意类型的参数，其语法在为：在 class 或 typename 后面加入省略号

Q: 内存泄漏
A:
![](res/2021-03-27-20-09-21.png)

Q: struct 和 class 的区别
A:


Q: STL如何删除元素
A:
![](res/2021-03-27-20-03-39.png)

Q:C++如何处理返回值
A:生成一个临时变量，将其引用作为函数参数传入函数内

Q:new/delete 和 malloc/free 的区别
A:
关键字和库函数的区别
malloc需要指定申请内存的大小，返回的指针需要强转
new会调用构造函数，不用指定内存大小，返回的指针不需要强转
![](res/2021-03-27-20-09-51.png)

Q: RTTI

Q:构造函数不能声明为虚函数
A:构造一个对象的时候，必须知道对象的实际类型，而虚函数行为是在运行期间确定实际类型的。而在构造一个对象时，由于对象还未构造成功。编译器无法知道对象 的实际类型，是该类本身，还是该类的一个派生类，或是更深层次的派生类。无法确定

虚函数的执行依赖于虚函数表。而虚函数表在构造函数中进行初始化工作，即初始化vptr，让他指向正确的虚函数表。而在构造对象期间，虚函数表还没有被初始化，将无法进行。虚函数的意思就是开启动态绑定，程序会根据对象的动态类型来选择要调用的方法。然而在构造函数运行的时候，这个对象的动态类型还不完整，没有办法确定它到底是什么类型，故构造函数不能动态绑定。（动态绑定是根据对象的动态类型而不是函数名，在调用构造函数之前，这个对象根本就不存在，它怎么动态绑定？）

Q: 堆和栈的区别
A:
![](res/2021-03-25-21-40-51.png)

Q: RAII机制
A:
RAII全称是“Resource Acquisition is Initialization”，直译过来是“资源获取即初始化”，也就是说在构造函数中申请分配资源，在析构函数中释放资源。因为C++的语言机制保证了，当一个对象创建的时候，自动调用构造函数，当对象超出作用域的时候会自动调用析构函数。所以，在RAII的指导下，我们应该使用类来管理资源，将资源和对象的生命周期绑定。
智能指针（std::shared_ptr和std::unique_ptr）即RAII最具代表的实现，使用智能指针，可以实现自动的内存管理，再也不需要担心忘记delete造成的内存泄漏。

Q: 虚函数和多态
A:
![](res/2021-03-27-19-54-34.png)

Q: 隐式类型转换
A:
![](res/2021-03-23-19-26-25.png)

Q:多态
A:
![](res/2021-03-23-19-24-39.png)

Q: 重载和重写
A:
![](res/2021-03-23-19-23-46.png)

Q: 析构函数的作用
A:
![](res/2021-03-23-19-22-10.png)

Q:静态函数和虚函数的区别
A:
![](res/2021-03-23-19-23-11.png)

Q: static关键字
A:
全局静态变量
局部静态变量
静态函数
类的静态成员
类的静态函数
![](res/2021-03-23-19-13-24.png)

Q: 怎么理解封装
A: 从形式上看，封装不过是将数据和行为组合在一个包内，并对对象的使用者隐藏了数据的实现形式。

Q: 怎么检测内存泄漏
A: 
valgrind：可以检测使用未初始化的内存 内存读写越界 动态内存管理 内存泄露

Q: C++ 与 C 的区别
A：
![](res/2021-03-23-19-10-48.png)

Q: 四种类型转换
A:
![](res/2021-03-23-19-14-09.png)

Q: 指针和引用的差别
A:
![](res/2021-03-23-19-15-18.png)

Q: 智能指针
A:
![](res/2021-03-23-19-16-19.png)
![](res/2021-03-23-19-16-49.png)
![](res/2021-03-23-19-17-18.png)
![](res/2021-03-23-19-17-55.png)

Q: 指针和数组的区别
A:
![](res/2021-03-23-19-18-31.png)

Q:为什么析构函数必须是虚函数
A:
![](res/2021-03-23-19-19-38.png)

Q:函数指针
A:
![](res/2021-03-23-19-20-24.png)

Q: C++11好处
A:


Q: 左值、右值
A:
C++中所有的值都必然属于左值、右值二者之一
左值是指表达式结束后依然存在的持久化对象，右值是指表达式结束时就不再存在的临时对象。所有的具名变量或者对象都是左值，而右值不具名。
有一个可以区分左值和右值的便捷方法：看能不能对表达式取地址，如果能，则为左值，否则为右值。

右值分为将亡值和纯右值。纯右值就是c++98标准中右值的概念，如非引用返回的函数返回的临时变量值；一些运算表达式，如1+2产生的临时变量；不跟对象关联的字面量值，如2，'c'，true，"hello"；这些值都不能够被取地址。
将亡值则是c++11新增的和右值引用相关的表达式，这样的表达式通常时将要移动的对象、T&&函数返回值、std::move()函数的返回值等，

左值引用：c++98中的引用，比如说
```C++
int a = 10; 
int& refA = a
```

右值引用
```C++
int&& a = 1;
A getTemp()
{
    return A();
}
A && a = getTemp();   //getTemp()的返回值是右值（临时变量）
```

移动构造函数：拷贝构造的参数是const MyString& str，是常量左值引用，而移动构造的参数是MyString&& str，是右值引用。右值，优先进入移动构造函数而不是拷贝构造函数。而移动构造函数与拷贝构造不同，它并不是重新分配一块新的空间，将要拷贝的对象复制过来，而是"偷"了过来，将自己的指针指向别人的资源，然后将别人的指针修改为nullptr

有些左值是局部变量，生命周期也很短，C++11为了解决这个问题，提供了std::move()方法来将左值转换为右值，从而方便应用移动语义。我觉得它其实就是告诉编译器，虽然我是一个左值，但是不要对我用拷贝构造函数，而是用移动构造函数

完美转发：转发，就是通过一个函数将参数继续转交给另一个函数进行处理，原参数可能是右值，可能是左值，如果还能继续保持参数的原有特征，那么它就是完美的。

std::forward()和universal references通用引用共同实现完美转发。

## Go

Q: Go 中 map 的底层实现
A: 总体来说golang的map是hashmap，是使用数组+链表的形式实现的，使用拉链法消除hash冲突。
golang的map有两种重要的结构，hmap和bmap。hmap中包含一个指向bmap数组的指针，key经过hash函数之后得到一个数，这个数低位用于选择bmap(当作bmap数组指针的下表)，高位用于放在bmap的[8]uint8数组中，用于快速试错。然后一个bmap可以指向下一个bmap(拉链)。
hmap是map的最外层的一个数据结构，包括了map的各种基础信息，存储的是指向buckets数组的一个指针。当bucket数组需要扩容时，它会开辟一倍的内存空间，并且会渐进式的把原数组拷贝，即用到旧数组的时候就拷贝到新数组。
bmap是数组元素，存储了三部分：高位哈希值、字节数组(存储key value)和指向下一个bmap(扩容用)的指针。
哈希表的特点是会有一个哈希函数，对你传来的key进行哈希运算，得到唯一的值，一般情况下都是一个数值。Golang的map中也有这么一个哈希函数，也会算出唯一的值。Golang把求得的值按照用途一分为二：高位和低位。结果的低位用于选择把KV放在bmap数组中的哪一个bmap中，高位用于key的快速预览，用于快速试错,把高八位存储起来，这样不用完整比较key就能过滤掉不符合的key，加快查询速度
我们传入的key和value放在bucket里面，注意，它的底层排列方式是，key全部放在一起，value全部放在一起。当key大于128字节时，bucket的key字段存储的会是指针，指向key的实际内容；value也是一样。这样排列好处是在key和value的长度不同的时候，可以消除padding带来的空间浪费。并且每个bucket最多存放8个键值对。
查找或者操作map时，首先key经过hash函数生成hash值，通过哈希值的低8位来判断当前数据属于哪个桶(bucket)，找到bucket以后，通过哈希值的高八位与bucket存储的高位哈希值循环比对，如果相同就比较刚才找到的底层数组的key值，如果key相同，取出value。如果高八位hash值在此bucket没有，或者有，但是key不相同，就去链表中下一个溢出bucket中查找，直到查找到链表的末尾。
碰撞冲突：如果不同的key定位到了统一bucket或者生成了同一hash,就产生冲突。 go是通过链表法来解决冲突的。比如一个高八位的hash值和已经存入的hash值相同，并且此bucket存的8个键值对已经满了，或者后面已经挂了好几个bucket了。那么这时候要存这个值就先比对key,key肯定不相同啊，那就从此位置一直沿着链表往后找，找到一个空位置，存入它。所以这种情况，两个相同的hash值高8位是存在不同bucket中的。
扩容：当链表越来越长，bucket的扩容次数达到一定值，其实是bmap扩容的加载因数达到6.5（元素个数/bucket），bmap就会进行扩容，将原来bucket数组数量扩充一倍，产生一个新的bucket数组，也就是bmap的buckets属性指向的数组。这样bmap中的oldbuckets属性指向的就是旧bucket数组。
加载因数6.5，这个是经过测试才得出的合理的一个阈值。因为，加载因子越小，空间利用率就小，加载因子越大，产生冲突的几率就大。所以6.5是一个平衡的值。
map的扩容不会立马全部复制，而是渐进式扩容，即首先开辟2倍的内存空间，创建一个新的bucket数组。只有当访问原来就的bucket数组时，才会将就得bucket拷贝到新的bucket数组，进行渐进式的扩容。当然旧的数据不会删除，而是去掉引用，等待gc回收。

![hmap bmap](res/2021-03-14-12-31-40.png)

Q:goroutine
A:
<https://studygolang.com/articles/32825>

Q: go语言中数组和切片的区别
A: 

数组是值类型：如果你将一个数组赋值给另外一个数组，那么，实际上就是将整个数组拷贝一份。值类型变量声明后，不管是否已经赋值，编译器为其分配内存，此时该值存储于栈上。
切片是引用类型：直接存放的就是一个内存地址值，这个地址值指向的空间存的才是值。所以修改其中一个，另外一个也会修改（同一个内存地址）。引用类型必须申请内存才可以使用，make()是给引用类型申请内存空间

数组的长度是固定的，而切片不是（切片是动态的数组），而且切片可以扩容。

在range开始迭代时，会拷贝一个副本，对数组来说，修改原数组不会修改被迭代的数组，而对切片来说，range拷贝的切片和原切片都指向同一底层数组，所以修改了原切片也会影响迭代的切片。

Q: go 切片底层实现
A: 通过指针引用底层数组，设定相关属性将数据读写操作限定在指定的区域内。切片本身是一个只读对象，其工作机制类似数组指针的一种封装。

Q: Go 中 channel 的底层实现
A: 

![channel](res/2021-03-14-14-13-14.png)
buf是有缓冲的channel所特有的结构，用来存储缓存数据。循环链表
sendx和recvx用于记录buf这个循环链表中的发送或者接收的index
lock是个互斥锁。
recvq和sendq分别是接收(<-channel)或者发送(channel <- xxx)的goroutine抽象出来的结构体(sudog)的队列。双向链表

recvx以及sendx会随着插入和弹出元素不断递增。每个操作都需要操作锁。

阻塞操作：当channel缓存满了，或者没有缓存的时候，这个时候G1正在正常运行,当再次进行send操作(ch<-1)的时候，会主动调用Go的调度器,让G1等待，并从让出M，让其他G去使用。同时G1也会被抽象成含有G1指针和send元素的sudog结构体保存到hchan的sendq中等待被唤醒。G2从缓存队列中取出数据，channel会将等待队列中的G1推出，将G1当时send的数据推到缓存中，然后调用Go的scheduler，唤醒G1，并把G1放到可运行的Goroutine队列中。
先进行执行recv操作的G2，这个时候G2会主动调用Go的调度器,让G2等待，并从让出M，让其他G去使用。G2还会被抽象成含有G2指针和recv空元素的sudog结构体保存到hchan的recvq中等待被唤醒。此时恰好有个goroutine G1开始向channel中推送数据 ch <- 1。G1并没有锁住channel，然后将数据放到缓存中，而是直接把数据从G1直接copy到了G2的栈中。在唤醒过程中，G2无需再获得channel的锁，然后从缓存中取数据。减少了内存的copy，提高了效率。

Q: golang判断channel是否已经close
A: 

读channel的时候判断其是否已经关闭
_,ok := <- jobs
写入channel的时候判断其是否已经关闭
再创建一个 channel，叫做 timeout，如果超时往这个 channel 发送 true，在生产者发送数据给 jobs 的 channel，用 select 监听 timeout，如果超时则关闭 jobs 的 channel

Q: Go 中 defer 和 return 执行的先后顺序
A: 多个defer的执行顺序为“后进先出”；defer、return、返回值三者的执行逻辑应该是：return最先执行，return负责将结果写入返回值中；接着defer开始执行一些收尾工作；最后函数携带当前返回值退出。

Q: 通道关闭后会怎么样
A: 
关闭后的通道有以下特点：
  - 对一个关闭的通道再发送值就会导致panic。
  - 对一个关闭的通道进行接收会一直获取值直到通道为空。
  - 对一个关闭的并且没有值的通道执行接收操作会得到对应类型的零值。
  - 关闭一个已经关闭的通道会导致panic。

![channel](res/2021-03-14-20-54-29.png)

Q: 业界常用的垃圾回收(GC)方法
A: 
引用计数：对每个对象维护一个引用计数，当引用该对象的对象被销毁时，引用计数减1，当引用计数器为0是回收该对象。
  优点：对象可以很快的被回收，不会出现内存耗尽或达到某个阀值时才回收。
  缺点：不能很好的处理循环引用，而且实时维护引用计数，有也一定的代价。
  代表语言：Python、PHP、Swift
标记-清除：从根变量开始遍历所有引用的对象，引用的对象标记为"被引用"，没有被标记的进行回收。
  优点：解决了引用计数的缺点。
  缺点：需要STW，即要暂时停掉程序运行。
  代表语言：Golang(其采用三色标记法)
分代收集：按照对象生命周期长短划分不同的代空间，生命周期长的放入老年代，而短的放入新生代，不同代有不能的回收算法和回收频率。
  优点：回收性能好
  缺点：算法复杂
  代表语言： JAVA

Q: 三色标记算法
A: 
灰色：对象已被标记，但这个对象包含的子对象未标记
黑色：对象已被标记，且这个对象包含的子对象也已标记，gcmarkBits对应的位为1（该对象不会在本次GC中被清理）
白色：对象未被标记，gcmarkBits对应的位为0（该对象将会在本次GC中被清理）
例如，当前内存中有A~F一共6个对象，根对象a,b本身为栈上分配的局部变量，根对象a、b分别引用了对象A、B, 而B对象又引用了对象D，则GC开始前各对象的状态如下图所示:
初始状态下所有对象都是白色的。
接着开始扫描根对象a、b; 由于根对象引用了对象A、B,那么A、B变为灰色对象，接下来就开始分析灰色对象，分析A时，A没有引用其他对象很快就转入黑色，B引用了D，则B转入黑色的同时还需要将D转为灰色，进行接下来的分析。
灰色对象只有D，由于D没有引用其他对象，所以D转入黑色。标记过程结束
最终，黑色的对象会被保留下来，白色对象会被回收掉。

三色标记最大的好处就是可以异步执行，从而可以以中断时间极少的代价或者完全没有中断来进行GC环节。
go语言采用写屏障解决漏标问题（黑色节点在执行过程中指向了白色节点）

## 数据结构

Q: 哈夫曼编码
A:
![](res/2021-03-29-21-35-56.png)

Q: 红黑树:
A:
![](res/2021-03-29-21-25-24.png)

![](res/2021-03-29-21-26-15.png)
（1）每个节点或者是黑色，或者是红色。
（2）根节点是黑色。
（3）每个叶子节点（NIL）是黑色。 [注意：这里叶子节点，是指为空(NIL或NULL)的叶子节点！]
（4）如果一个节点是红色的，则它的子节点必须是黑色的。
（5）对于任意节点而言，其到叶子点数NULL指针的每条路径都包含相同数目的黑节点

![](res/2021-03-29-21-27-18.png)

对x进行左旋，意味着，将“x的右孩子”设为“x的父亲节点”；即，将 x变成了一个左节点(x成了为z的左孩子)！。 因此，左旋中的“左”，意味着“被旋转的节点将变成一个左节点”。
![](res/2021-03-05-15-01-31.png)

对x进行右旋，意味着，将“x的左孩子”设为“x的父亲节点”；即，将 x变成了一个右节点(x成了为y的右孩子)！ 因此，右旋中的“右”，意味着“被旋转的节点将变成一个右节点”。
![](res/2021-03-05-15-01-47.png)
![](res/2021-03-29-21-27-58.png)

Q：排序稳定性
A: 
待排序的序列中有两元素相等,排序之后它们的先后顺序不变。

冒泡排序(稳定)与快速排序（不稳定）都属于交换排序。冒泡排序是通过不停的遍历,以升序为例,如果相邻元素中左边的大于右边的则交换.碰到相等的时就不交换保持原位.所以冒泡排序是一种稳定排序算法。快排对于 [1, 2, 2]，若选择a[2]（即数组中的第二个2）为比较子，而把大于等于比较子的数均放置在大数数组中，则a[1]（即数组中的第一个2）会到pivot的右边，那么数组中的两个2非原序。

简单选择排序(不稳定)与堆排序(不稳定)都属于选择排序。（从待排序的元素中挑选出最大或最小值）简单选择排序选出最小值后需要交换位置。对于[8, 3, 8, 1]，当从左往右遍历找最小值时,找到了1,这就需要把8跟1交换.这样两个相等元素8的位置就变了。堆排序的话,也会存在跟上面一样的交换最大值的位置会导致不稳定.例如有大堆 8 8 6 5 2.先选出第一个最大值8,放最末尾.此时就不稳定了.因为第二个8就跑它前面去了。

插入排序(稳定)：先在已排序好的的子序列中找到合适的位置再插入。假设左边是已排序的,右边是没排序的.通过从后向前遍历已排序序列,然后插入,此时相等元素依然可以保持原有位置。二分插入排序是不稳定的,因为通过二分查找时得到的位置不稳定。

归并排序(稳定)使得了递归的思想,把序列递归的分割成小序列,然后合并排好序的子序列.当有序列的左右两子序列合并的时候一般是先遍历左序列,所在左右序列如果有相等元素,则处在左边的仍然在前,这就稳定了。但是如果你非得先遍历右边序列则算法变成不稳定的了，虽然这样排出来的序也是对的,但变成了不稳定的,所以是不太好的实现。

希尔排序(不稳定)是按照不同步长对元素进行插入排序，当刚开始元素很无序的时候，步长最大，所以插入排序的元素个数很少，速度很快；当元素基本有序了，步长很小， 插入排序对于有序的序列效率很高。所以，希尔排序的时间复杂度会比O(n^2)好一些。由于多次插入排序，我们知道一次插入排序是稳定的，不会改变相同元素的相对顺序，但在不同的插入排序过程中，相同的元素可能在各自的插入排序中移动，最后其稳定性就会被打乱，所以shell是一种不稳定排序算法。

基数排序(稳定)是按照低位先排序，然后收集；再按照高位排序，然后再收集；依次类推，直到最高位。有时候有些属性是有优先级顺序的，先按低优先级排序，再按高优先级排序，最后的次序就是高优先级高的在前，高优先级相同的低优先级高的在前。基数排序基于分别排序，分别收集，所以其是稳定的排序算法。

选择排序、快速排序、希尔排序、堆排序不是稳定的排序算法；
冒泡排序、插入排序、归并排序和基数排序是稳定的排序算法。

![Sort](res/2021-03-19-14-28-29.png)

基本有序时或逆序时快速排序慢；基本有序时插入、希尔、冒泡快；逆序时冒泡排序慢
计数排序很好用，若是整数且范围小，请用计数排序
求前k个用堆排序或选择排序

Q: hash冲突
A:
![](res/2021-03-24-11-30-13.png)

Q: STL哈希
A:
![](res/2021-03-29-20-57-38.png)

Q: TopK
A:
1.全排序 O(n*lg(n))
2.局部排序（冒泡排序）O(n*k)
3.优先队列 O(n*lg(k))
4.随机选择+partition

无法装入内存
分治法：将数据分为N份，保证每一份数据都能装载进内存，每一份得到前k个，合为 N*k 个数据，然后再拆。

数据范围较小：bitmap hash 计数排序

Q: 数组和链表
A:
![](res/2021-03-29-21-13-59.png)
![](res/2021-03-29-21-14-35.png)
![](res/2021-03-29-21-14-55.png)

## 计算机网络

Q:CDN
A:
CDN即内容分发网络（Content Delivery Network）的简称，是建立在承载网基础上的虚拟分布式网络，能够将源站内容（包括各类动静态资源）智能缓存到全球各节点服务器上。这样不仅方便了用户就近获取内容，提高了资源的访问速度，也分担了源站压力。

CDN缓存节点可分为L1节点和L2节点。L1节点分布在全国各省市，L2节点分布在几个大区下
CDN节点缓存策略如下：
客户端在请求域名时，先向本地DNS查询该域名对应的IP地址，本地DNS再向权威DNS进行查询，由阿里云CDN进行调度，为该DNS分配对应的节点。
客户端向CDN节点发起连接请求，当L1节点有缓存资源时，会命中该资源，直接将数据返回给客户端。当L1节点无缓存资源时，会向L2节点请求对应资源，如果L2节点有缓存资源，则将资源同步到L1节点，并返回给用户；如果L2节点无缓存资源，则直接回客户源站获取资源，并按照配置的缓存策略进行缓存。

CDN工作原理：假设您的加速域名为www.a.com，接入CDN网络，开始使用加速服务后，当终端用户（北京）发起HTTP请求时，处理流程如下图所示。
![](res/2021-03-27-19-21-44.png)
1.当终端用户（北京）向www.a.com下的某资源发起请求时，首先向LDNS（本地DNS）发起域名解析请求。
2.LDNS检查缓存中是否有www.a.com的IP地址记录。如果有，则直接返回给终端用户；如果没有，则向授权DNS查询。
3.当授权DNS解析www.a.com时，返回域名CNAME www.a.tbcdn.com对应IP地址。
4.域名解析请求发送至阿里云DNS调度系统，并为请求分配最佳节点IP地址。
5.LDNS获取DNS返回的解析IP地址。
6.用户获取解析IP地址。
7.用户向获取的IP地址发起对该资源的访问请求。

Q:协议头部
A:
TCP
![](res/2021-03-27-15-48-13.png)
序列号，它是TCP报文段的一数字编号，为保证TCP可靠连接，每一个发送的数据段都要加上序列号。建立连接时，两端都会随机生成一个初始序列号。而确认号是和序列号配合使用的，应答某次请求时，则返回一个确认号，它的值等于对方请求序列号加1
6个标志位分别是，URG：这是条紧急信息，ACK:应答消息，PSH:缓冲区尚未填满，RST:重置连接，SYN:建立连接消息标志，FIN：连接关闭通知信息
窗口大小是接收端用来控制发送端的滑动窗口大小

UDP
![](res/2021-03-27-16-05-04.png)
UDP首部有8个字节，由4个字段构成，每个字段都是两个字节，
1.源端口： 源端口号，需要对方回信时选用，不需要时全部置0.
2.目的端口：目的端口号，在终点交付报文的时候需要用到。
3.长度：UDP的数据报的长度（包括首部和数据）其最小值为8（只有首部）
4.校验和：检测UDP数据报在传输中是否有错，有错则丢弃。
计算校验和的时候，需要在UDP数据报之前增加12字节的伪首部，伪首部并不是UDP真正的首部。只是在计算校验和，临时添加在UDP数据报的前面，得到一个临时的UDP数据报。校验和就是按照这个临时的UDP数据报计算的。伪首部既不向下传送也不向上递交，而仅仅是为了计算校验和。这样的校验和，既检查了UDP数据报，又对IP数据报的源IP地址和目的IP地址进行了检验。
![](res/2021-03-27-16-08-09.png)

![](res/2021-03-27-16-12-19.png)
IP协议
版本号：占4位，指IP协议的版本，有IPv4和IPv6两种，对于IPv4协议该字段就是4
首部长度：占4位，指IP协议的报头长度，范围在20-60字节
区分服务：占8位，只有在使用区分服务时该字段才有作用，一般情况下不使用该字段
长度：占16位，指IP协议的总长度：报头长度+数据长度
根据IP报头来看，长度占16位，长度应该在65535个字节，但实际传送不了这么大长度的IP报文；
IP报文的长度是依据下层数据链路层规定的数据帧中的数据字段的最大长度，称为最大传输单元MTU；
一般最大传输单元MTU的长度为1500个字节，所以IP报文最大长度为1500个字节，若超过该长度，IP报文会被分片。

16位标识、3位标志、13位片偏移用于IP报文的分片和组装
标识：占16位，由于IP报文有长度的限制，超过MTU会被分片，而在交付给上层时需要将IP报文组装，该字段用于唯一标识一个IP报文
标志：占3位，第一位为保留位，第二位表示该IP报文是否分片（1表示禁止分片，0表示分片），第三位表示该IP报文是否为最后一个
片偏移：占13位，在报文分片时有效，表示该片报文应位于整个IP报文的那个位置

生存时间TTL：占8位，在路由时通过一跳与多跳的方式查找路径，为防止某个报文在网络中一直处于游离状态，无限循环，所以在报头中规定了报文在网络中最多经过路由器的数量，也就是该报文的最大跳数
协议类型：占8位，表示该IP报文要交给上层的那个协议（UDP或TCP）
首部校验和：占16位，鉴别头部是否有损坏
源IP：占32位，表示该IP报文从哪里来
目的IP：占32位，表示该IP报文要发送到哪里

HTTP请求报文与响应报文格式
请求报文包含三部分：
a、请求行：包含请求方法、URI、HTTP版本信息
b、请求首部字段
c、请求内容实体
响应报文包含三部分：
a、状态行：包含HTTP版本、状态码、状态码的原因短语
b、响应首部字段
c、响应内容实体

Q: http长连接
A:
长连接是指客户端和服务建立TCP连接后，它们之间的连接会持续存在，不会因为一次HTTP请求后关闭，后续的请求也是用这个连接
长连接可以省去TCP的建立和关闭操作，对于频繁请求的客户端适合使用长连接，但是注意恶意的长连接导致服务受损（建议内部服务之间使用）

Q: 大端小端
A:
大端模式，就是高位字节排放在内存的低地址端，低位字节排放在内存的高地址端。
小端模式，就是低位字节排放在内存的低地址端，高位字节排放在内存的高地址端。

X86结构是小端模式，网络字节序是大端字节序;

类型强转是取低地址，如果是小端模式取低字节，大端模式取高字节

一般都是通过 union 判断机器的字节序
union 型数据所占的空间等于其最大的成员所占的空间。对 union 型的成员的存取都是相对于该联合体基地址的偏移量为 0 处开始，也就是联合体的访问不论对哪个变量的存取都是从 union 的首地址位置开始。
联合是一个在同一个存储空间里存储不同类型数据的数据类型。这些存储区的地址都是一样的，联合里不同存储区的内存是重叠的，修改了任何一个其他的会受影响。
```C++
#include <stdio.h>
 int main (void)
 {
	union
	{
 		short i;
 		char a[2];
 	}u;
 	u.a[0] = 0x11;
 	u.a[1] = 0x22;
 	printf ("0x%x\n", u.i);  
 	//0x2211 为小端  0x1122 为大端
 	return 0;
 }
```

Q: Linux与Windows下的网络编程区别
A: 

头文件：Windows下winsock.h/winsock2.h Linux下sys/socket.h
初始化：Windows下需要用WSAStartup Linux下不需要
关闭socket：closesocket(…) Linux下close(…)
设置非阻塞：Windows下ioctlsocket() Linux下fcntl() <fcntl.h>

linux下的socket编程：
1、客户端执行步骤依次如下:
socket()
connect()
send()或者recv()
close()
注意的是，connect之前要填充地址结构体，IP地址转换为网络字节序，一般用inet_aton().

2、服务器端：
socket()
bind()
listen()
accpet()
recv()或者send()
close()
(ps:一般通过将send()和recv()的最后一个参数赋为0或者1来区分阻塞与非阻塞，其中0对应阻塞，1对应非阻塞)

Q: Session、Cookie和Token的主要区别
A:
cookie是由Web服务器保存在用户浏览器上的小文件（key-value格式），包含用户相关的信息。客户端向服务器发起请求，如果服务器需要记录该用户状态，就使用response向客户端浏览器颁发一个Cookie。客户端浏览器会把Cookie保存起来。当浏览器再请求该网站时，浏览器把请求的网址连同该Cookie一同提交给服务器。服务器检查该Cookie，以此来辨认用户身份。

session是依赖Cookie实现的。session是服务器端对象
session 是浏览器和服务器会话过程中，服务器分配的一块储存空间。服务器默认为浏览器在cookie中设置 sessionid，浏览器在向服务器请求过程中传输 cookie 包含 sessionid ，服务器根据 sessionid 获取出会话中存储的信息，然后确定会话的身份信息。

cookie与session区别
存储位置与安全性：cookie数据存放在客户端上，安全性较差，session数据放在服务器上，安全性相对更高；
存储空间：单个cookie保存的数据不能超过4K，很多浏览器都限制一个站点最多保存20个cookie，session无此限制
占用服务器资源：session一定时间内保存在服务器上，当访问增多，占用服务器性能，考虑到服务器性能方面，应当使用cookie。
存储方式不同，cookie中只能保管ASCII字符串；session中能够存储任何类型的数据
跨域支持上不同，cookie支持跨域名访问；session不支持跨域名访问
隐私策略不同，cookie对客户端是可见的；session存储在服务器上，对客户端是透明

Token是在客户端频繁向服务端请求数据，服务端频繁的去数据库查询用户名和密码并进行对比，判断用户名和密码正确与否，并作出相应提示，在这样的背景下，Token便应运而生。
Token的定义：Token是服务端生成的一串字符串，以作客户端进行请求的一个令牌，当第一次登录后，服务器生成一个Token便将此Token返回给客户端，以后客户端只需带上这个Token前来请求数据即可，无需再次带上用户名和密码。
使用Token的目的：Token的目的是为了减轻服务器的压力，减少频繁的查询数据库，使服务器更加健壮。
Token 是在服务端产生的。如果前端使用用户名/密码向服务端请求认证，服务端认证成功，那么在服务端会返回 Token 给前端。前端可以在每次请求的时候带上 Token 证明自己的合法地位

session与token区别
session机制存在服务器压力增大，CSRF跨站伪造请求攻击，扩展性不强等问题；
session存储在服务器端，token存储在客户端
token提供认证和授权功能，作为身份认证，token安全性比session好；
session这种会话存储方式方式只适用于客户端代码和服务端代码运行在同一台服务器上，token适用于项目级的前后端分离（前后端代码运行在不同的服务器下）

Q: 数据链路层成帧
A:
帧的组成主要是标志和字段两个部分，标志主要是标识了帧的开始和结束，字段部分主要有地址字段，控制字段，正文字段和校验字段四个部分。地址字段表明了帧的去向和来源，这是硬件的网卡地址，控制字段就是各种协议，正文字段是真正的信息，校验字段是用来检验帧是不是有错误，通常有CRC校验等等。标志是成帧的一个重要标志，链路层读到标志，就知道帧开始了，这也就界定了一个帧的范围。对于正文字段，链路层读不懂，他也不会在意正文字段是什么
数据链路层成帧的方法主要有三个：字符计数法，字符填充的首尾界定法和比特填充的首尾界定法。
1.字符计数法 用一个帧的第一字节来说明帧的总长度（总长度包含这个帧头）
2.字符填充的首尾界定法 在帧的头之前和尾之后加一个特殊的字符，只要读到这个字符帧就开始了，再次读到就认为这个帧结束了
3.比特填充的字符界定法 和第二种比较类似，区别是他把flag具体化了，为6个1。这样当正文读取的时候一旦出现了5个连续的1，那么在后面填充一个0，避免出现6个1造成帧提前结束

Q: 数据从网卡到应用层的过程
A:
1.网卡收到的数据是 光信号或电信号，然后将其还原成 数字信息
2.校验数据，判断数据在传输过程是否因噪音等影响导致信号失真，从而导致数据错误，需要丢弃这种无效的数据包。
3.检查 数据包中MAC头部中的 接收方的MAC地址，若不是发给自己，则丢弃数据包；若数据包是发给自己，则将数字信息保存到网卡内部缓冲区。
4.网卡通过中断将数据包达到的事件通知给CPU。接着，CPU暂停手头工作，开始用网卡驱动来干活
5.CPU从网卡缓冲区读取接收到的数据，根据MAC头部的以太类型字段判断协议种类并调用处理该协议的软件(即协议栈)
6.当MAC头部以太类型为 IP 协议时，网卡驱动数据包交给TCP/IP协议栈来处理。
7.IP模块会检查IP头部以判断数据是不是发给自己。判断数据包是否分片，如果分片则缓存起来等待分片全部到达再还原成数据包。根据IP头部的协议号字段，将包转给TCP模块或UDP模块处理
8.TCP模块会根据 标志位 来进行不同处理
    SYN=1：请求连接，首先检查接收方端口号，然后检查有没有与该端口号相同且处于等待连接状态的套接字。如果没有，则返回错误通知的包；如果有，则为这个套接字复制一个新副本，将发送方IP、端口等必要信息写入套接字，同时分配用于发送缓冲区和接收缓冲区的内存空间。最后返回给数据客户端，客户端会再次确认，这属于TCP连接三次握手的一部分
    正常数据包，TCP模块需要检查该包对应的套接字。然后提取出数据，存放到缓冲区。此时，如果应用程序调用socket的read()，数据就可以转交给应用程序了。如果应用程序不来获取数据，数据则一直保存在缓冲区中。

多路复用
首先，应用告诉内核对每个套接字感兴趣的事件，例如：socket1的读事件。
当客户端发送数据过来时，内核从网卡读取数据，然后调用socket1回调函数，将socket1作为可读事件加入事件列表
应用层获取读写事件列表时，拿到的就是实际可读写的事件，没有无效数据。

Q: http发展史
A:
主要就是为了将超文本标记语言(HTML)文档从Web服务器传送到客户端的浏览器

HTTP1.1在HTTP1.0基础上的改进
1.长连接
在一个TCP连接上可以传送多个HTTP请求和响应，减少了建立和关闭连接的消耗和延迟。HTTP 1.0需要使用keep-alive参数来建立一个长连接，而HTTP1.1默认支持长连接
长连接的好处：一个网页上可能有多个资源对象，长连接可以通过一个连接传输网页上的所有对象，而短连接每次连接只能传输一个对象，也就是一个网页的内容需要传输多次
2.缓存
HTTP1.0缓存的资源对象到了一定时间之后会失效，不能再次使用；而HTTP1.1缓存的资源对象失效后还能与源服务器进行重新激活。
3.带宽使用
HTTP/1.0一次只能请求一整个资源对象，而HTTP/1.1可以请求一个资源对象的一部分，因此在不需要得到整个资源对象时，可节约带宽，而且支持断点续传
4.Host域
由于一台物理服务器上可以存在多个虚拟主机，并且它们共享一个IP地址，因此HTTP1.1在HTTP1.0的基础上加了改进，加了一个Host域，用于指定共享同一个IP地址中的某一台主机，而HTTP1.0则默认一个IP地址只能属于一台主机，没有Host域
5.错误通知的管理，在HTTP1.1中新增了24个错误状态响应码

HTTP2.0在HTTP1.1基础上的改进
HTTP/2 采用二进制格式传输数据，而非HTTP/1.x 里纯文本形式的报文 ，二进制协议解析起来更高效。HTTP/2 将请求和响应数据分割为更小的帧，并且它们采用二进制编码。
1.多路复用
HTTP2.0同一个连接可以并发处理多个请求，而且并发请求的数量比HTTP1.1大了好几个数量级，这意味着减少了建立连接所需要的开销
2.数据压缩
HTTP的请求和响应包括三个部分，即状态行，头部信息，消息主体。HTTP1.1只对消息主体进行压缩，而HTTP2.0对状态行，头部信息，消息主体都进行压缩
3.服务器推送
在使用HTTP1.1时，客户端请求什么资源，服务器才给什么；而HTTP2.0服务器会自动把客户端一定需要的资源传输给客户端，比如一些必要的附加资源等等

Q: HTTP2.0的多路复用和HTTP1.X中的长连接复用有什么区别？
A: HTTP/1.* 一次请求-响应，建立一个连接，用完关闭；每一个请求都要建立一个连接；
HTTP/1.1 Pipeling解决方式为，若干个请求排队串行化单线程处理，后面的请求等待前面请求的返回才能获得执行机会，一旦有某请求超时等，后续请求只能被阻塞，毫无办法，也就是人们常说的线头阻塞；
HTTP/2多个请求可同时在一个连接上并行执行。某个请求任务耗时严重，不会影响到其它连接的正常执行

Q: Http与Https的区别
A:
HTTP协议运行在TCP之上，所有传输的内容都是明文，HTTPS运行在SSL/TLS之上，SSL/TLS运行在TCP之上，所有传输的内容都经过加密的。
HTTP 的URL 以http:// 开头，而HTTPS 的URL 以https:// 开头
HTTP 是不安全的，而 HTTPS 是安全的
HTTP 标准端口是80 ，而 HTTPS 的标准端口是443
在OSI 网络模型中，HTTP工作于应用层，而HTTPS 的安全传输机制工作在传输层
HTTP 无法加密，而HTTPS 对传输的数据进行加密
HTTP无需证书，而HTTPS 需要CA机构wosign的颁发的SSL证书

Q: 什么是Http协议无状态协议?怎么解决Http协议无状态协议?
A:
无状态协议对于事务处理没有记忆能力。缺少状态意味着如果后续处理需要前面的信息
也就是说，当客户端一次HTTP请求完成以后，客户端再发送一次HTTP请求，HTTP并不知道当前客户端是一个”老用户“。
可以使用Cookie来解决无状态的问题，Cookie就相当于一个通行证，第一次访问的时候给客户端发送一个Cookie，当客户端再次来的时候，拿着Cookie(通行证)，那么服务器就知道这个是”老用户“。

Q: http状态码
A: 
当浏览者访问一个网页时，浏览者的浏览器会向网页所在服务器发出请求。当浏览器接收并显示网页前，此网页所在的服务器会返回一个包含HTTP状态码的信息头（server header）用以响应浏览器的请求。
1**     信息，服务器收到请求，需要请求者继续执行操作
2**     成功，操作被成功接收并处理
3**     重定向，需要进一步的操作以完成请求
4**     客户端错误，请求包含语法错误或无法完成请求
5**     服务器错误，服务器在处理请求的过程中发生了错误

200:OK请求成功
204:No Content无内容。服务器成功处理，但未返回内容。
206:Partial Content部分内容。服务器成功处理了部分GET请求
301:Moved Permanently请求的网页已永久移动到新位置,返回信息会包括新的URI，浏览器会自动定向到新URI。今后任何新的请求都应使用新的URI代替
400:Bad Request客户端请求的语法错误，服务器无法理解
401:Unauthorized请求要求用户的身份认证
403:Forbidden服务器理解请求客户端的请求，但是拒绝执行此请求
404:Not Found服务器无法根据客户端的请求找到资源（网页）。
405:Method Not Allowed客户端请求中的方法被禁止
500:Internal Server Error服务器内部错误，无法完成请求
501:Not Implemented服务器不支持请求的功能，无法完成请求
502:Bad Gateway服务器作为网关或代理，从上游服务器收到无效响应。
503:Service Unavailable由于超载或系统维护，服务器暂时的无法处理客户端的请求
504:Gateway Time-out充当网关或代理的服务器，未及时从远端服务器获取请求

Q: 常用的HTTP方法有哪些?
A:
GET： 用于请求访问已经被URI（统一资源标识符）识别的资源，可以通过URL传参给服务器
POST：用于传输信息给服务器，主要功能与GET方法类似，但一般推荐使用POST方式。
PUT： 传输文件，报文主体中包含文件内容，保存到对应URI位置。
HEAD： 获得报文首部，与GET方法类似，只是不返回报文主体，一般用于验证URI是否有效。
DELETE：删除文件，与PUT方法相反，删除对应URI位置的文件。
OPTIONS：查询相应URI支持的HTTP方法。

Q: GET方法与POST方法的区别
A:
get重点在从服务器上获取资源，post重点在向服务器发送数据；
get是不安全的，因为URL是可见的，可能会泄露私密信息，如密码等；post较get安全性较高；post传输数据通过Http的post机制，将字段与对应值封存在请求实体中发送给服务器，这个过程对用户是不可见的；
Get传输的数据量小，因为受URL长度限制，但效率较高；;Post可以传输大量数据，所以上传文件时只能用Post方式；
get方式只能支持ASCII字符，向服务器传的中文字符可能会乱码;post支持标准字符集，可以正确传递中文字符。

Q: URI和URL的区别
A:
URI，是uniform resource identifier，统一资源标识符，用来唯一的标识一个资源。
Web上可用的每种资源如HTML文档、图像、视频片段、程序等都是一个来URI来定位的
URI一般由三部组成：
①访问资源的命名机制
②存放资源的主机名
③资源自身的名称，由路径表示，着重强调于资源。

URL是uniform resource locator，统一资源定位器，它是一种具体的URI，即URL可以用来标识一个资源，而且还指明了如何locate这个资源。
采用URL可以用一种统一的格式来描述各种信息资源，包括文件、服务器的地址和目录等。URL一般由三部组成：
①协议(或称为服务方式)
②存有该资源的主机IP地址(有时也包括端口号)
③主机资源的具体地址。如目录和文件名等

Q:TCP 如何保证可靠性
A:
校验和
序列号
确认应答:在TCP的首部中有一个标志位——ACK，此标志位表示确认号是否有效。接收方对于按序到达的数据会进行确认，当标志位ACK=1时确认首部的确认字段有效。进行确认时，确认字段值表示这个值之前的数据都已经按序到达了。而发送方如果收到了已发送的数据的确认报文，则继续传输下一部分数据；而如果等待了一定时间还没有收到确认报文就会启动重传机制。
超时重传：当报文发出后在一定的时间内未收到接收方的确认，发送方就会进行重传
两种情况：发送包丢失、ack包丢失（当接收方接收到重复的数据时就将其丢掉，重新发送ACK。）
连接管理：三次握手和四次挥手
流量控制和拥塞控制机制

通过 TCP 连接传输的数据无差错，不丢失，不重复，且按顺序到达。
TCP 报文头里面的序号能使 TCP 的数据按序到达
报文头里面的确认序号能保证不丢包，累计确认及超时重传机制
TCP 拥有流量控制及拥塞控制的机制


a、通用首部字段（请求报文与响应报文都会使用的首部字段）
Date：创建报文时间
Connection：连接的管理
Cache-Control：缓存的控制
Transfer-Encoding：报文主体的传输编码方式
b、请求首部字段（请求报文会使用的首部字段）
Host：请求资源所在服务器
Accept：可处理的媒体类型
Accept-Charset：可接收的字符集
Accept-Encoding：可接受的内容编码
Accept-Language：可接受的自然语言
c、响应首部字段（响应报文会使用的首部字段）
Accept-Ranges：可接受的字节范围
Location：令客户端重新定向到的URI
Server：HTTP服务器的安装信息
d、实体首部字段（请求报文与响应报文的的实体部分使用的首部字段）
Allow：资源可支持的HTTP方法
Content-Type：实体主类的类型
Content-Encoding：实体主体适用的编码方式
Content-Language：实体主体的自然语言
Content-Length：实体主体的的字节数
Content-Range：实体主体的位置范围，一般用于发出部分请求时使用

Q: HTTP协议的优化方法
A:
TCP复用：TCP连接复用是将多个客户端的HTTP请求复用到一个服务器端TCP连接上，而HTTP复用则是一个客户端的多个HTTP请求通过一个TCP连接进行处理。前者是负载均衡设备的独特功能；而后者是HTTP 1.1协议所支持的新功能，目前被大多数浏览器所支持。
内容缓存：将经常用到的内容进行缓存起来，那么客户端就可以直接在内存中获取相应的数据了。
压缩：将文本数据进行压缩，减少带宽
SSL加速（SSL Acceleration）：使用SSL协议对HTTP协议进行加密，在通道内加密并加速
TCP缓冲：通过采用TCP缓冲技术，可以提高服务器端响应时间和处理效率，减少由于通信链路问题给服务器造成的连接负担。

Q: Reactor 反应堆设计模式
A:
最原始的网络编程思路是服务器使用一个while循环并不断监听端口是否有新的socket套接字连接，如果有就会去调用一个处理函数。这种方式最大的问题是无法并发且效率太低，如果当前请求没有处理完毕后续请求只能被阻塞，因此服务器的吞吐量太低。
多线程并发模式采用一个连接一个线程的方式，优点是确实一定程度上提高了服务器的吞吐量，因为之前的请求在read读阻塞后不会影响到后续的请求，由于它们在不同的线程中，而且一个线程只能对应一个套接字socket，每一个套接字socket都是阻塞的，所以一个线程中只能处理一个套接字。
对此考虑使用线程池使得线程可以被复用，大大降低创建线程和销毁线程的时间。然而，线程池并不能很好满足高并发线程的需求。当海量请求抵达时线程池中的工作线程达到饱和状态，此时可能就导致请求被抛弃，无法完成客户端的请求。
Reactor模式是基于事件驱动模型，当接收到请求后会将请求封装成事件，并将事件分发给相应处理事件的handler，handler处理完成后将时间状态修改为下一个状态，再由Reactor将事件分发给能够处理下一个状态的handle进行处理
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。
Rector模式指出在等待IO时，线程可以先退出，这样就会因为有线程等待IO而占用资源。但是这样原先的执行流程就没法还原了。因此可以利用事件驱动的方式，要求线程在退出之前向event loop事件循环中注册回调函数，这样IO完成时event loop事件循环就可以调用回调函数完成剩下的操作。所以Reactor模式通过减少服务器的资源消耗提供并发能力。
典型的事件包括连接、读取、写入，因此需要为这些事件分别提供对应的处理程序，每个处理程序可以采用线程的方式实现。一旦连接来了，而且显示被读取线程或处理程序处理了，则会再执行写入。那么之前的读取就可以被后面的请求复用

Q: ICMP 报文
A:
IP协议没有提供检测网络层是否畅通的功能，如果丢包了，IP协议并不能通知传输层是否丢包以及丢包的原因。ICMP协议是对IP协议的补充。ICMP是基于IP协议工作的，但是它并不是传输层的功能，因此仍然把它归结为网络层协

ICMP协议的功能主要有：
1.确认IP包是否成功到达目标地址
2.通知在发送过程中IP包被丢弃的原因

两种ICMP报文：差错通知和信息查询

ICMP报文包含在IP数据报中，IP报头在ICMP报文的最前面。一个ICMP报文包括IP报头（至少20字节）、ICMP报头（至少八字节）和ICMP报文（属于ICMP报文的数据部分）。当IP报头中的协议字段值为1时，就说明这是一个ICMP报文。

ping
一台主机向一个节点发送一个类型字段值为8的ICMP报文，如果途中没有异常（如果没有被路由丢弃，目标不回应ICMP或者传输失败），则目标返回类型字段值为0的ICMP报文，说明这台主机存在。
ping命令的功能
（1）能验证网络的连通性
（2）会统计响应时间和TTL(IP包中的Time To Live，生存周期)

目标不可达
我们要连接对方一个不存在的系统端口（端口号小于1024）时，将返回类型字段值3、代码字段值为3的ICMP报文。常见的不可到达类型还有网络不可到达（代码字段值为0）、主机不可达到（代码字段值为1）、协议不可到达（代码字段值为2）等等。

源抑制报文（类型字段值为4，代码字段值为0）
控制流量，通知主机减少数据报流量。由于ICMP没有回复传输的报文，所以只要停止该报文，主机就会逐渐恢复传输速率。

超时报文
无连接方式网络的问题就是数据报会丢失，或者长时间在网络游荡而找不到目标，或者拥塞导致主机在规定的时间内无法重组数据报分段，这时就要触发ICMP超时报文的产生。

时间戳请求
时间戳请求报文（类型值字段13）和时间戳应答报文（类型值字段14）用于测试两台主机之间数据报来回一次的传输时间。
传输时，主机填充原始时间戳，接受方收到请求后填充接受时间戳后以类型值字段14的报文格式返回，发送方计算这个时间差。

traceroute
利用ICMP差错控制报文中的TTL超时会回向源点发送一个时间超时报文。例如A 主机 traceroute B主机，A会封装一些分组，这些分组很特殊，例如第一个分组的TTL设置为1 ，第二个分组的TTL设置为2 以此类推.......当第一个分组到达第一个路由器时，发现TTL变成了0就会给源主机发送一个时间超时报文，这也就知道了这个分组所经过的一个路由器，同理可得。 当最后一个分组到达B主机时。收集每个时间超时的报文中的IP 就获得了A主机到B主机的路径。

Q:三次握手
A:
三次握手（Three-way Handshake）其实就是指建立一个TCP连接时，需要客户端和服务器总共发送3个包。进行三次握手的主要作用就是为了确认双方的接收能力和发送能力是否正常、指定自己的初始化序列号为后面的可靠性传送做准备。实质上其实就是连接服务器指定端口，建立TCP连接，并同步连接双方的序列号和确认号，交换TCP窗口大小信息。
刚开始客户端处于 Closed 的状态，服务端处于 Listen 状态。
进行三次握手：
第一次握手：客户端给服务端发一个 SYN 报文，并指明客户端的初始化序列号 ISN©。此时客户端处于 SYN_SEND 状态。
首部的同步位SYN=1，初始序号seq=x，SYN=1的报文段不能携带数据，但要消耗掉一个序号。
第二次握手：服务器收到客户端的 SYN 报文之后，会以自己的 SYN 报文作为应答，并且也是指定了自己的初始化序列号 ISN(s)。同时会把客户端的 ISN + 1 作为ACK 的值，表示自己已经收到了客户端的 SYN，此时服务器处于 SYN_REVD 的状态。
在确认报文段中SYN=1，ACK=1，确认号ack=x+1，初始序号seq=y。
第三次握手：客户端收到 SYN 报文之后，会发送一个 ACK 报文，当然，也是一样把服务器的 ISN + 1 作为 ACK 的值，表示已经收到了服务端的 SYN 报文，此时客户端处于 ESTABLISHED 状态。服务器收到 ACK 报文之后，也处于 ESTABLISHED 状态，此时，双方已建立起了连接。
确认报文段ACK=1，确认号ack=y+1，序号seq=x+1（初始为seq=x，第二个报文段所以要+1），ACK报文段可以携带数据，不携带数据则不消耗序号。
发送第一个SYN的一端将执行主动打开（active open），接收这个SYN并发回下一个SYN的另一端执行被动打开（passive open）。
在socket编程中，客户端执行connect()时，将触发三次握手。
![](res/2021-03-23-21-13-38.png)

第一次握手：客户端发送网络包，服务端收到了。
这样服务端就能得出结论：客户端的发送能力、服务端的接收能力是正常的。
第二次握手：服务端发包，客户端收到了。
这样客户端就能得出结论：服务端的接收、发送能力，客户端的接收、发送能力是正常的。不过此时服务器并不能确认客户端的接收能力是否正常。
第三次握手：客户端发包，服务端收到了。
这样服务端就能得出结论：客户端的接收、发送能力正常，服务器自己的发送、接收能力也正常。
因此，需要三次握手才能确认双方的接收与发送能力是否正常。

主要是防止已过期的连接再次传到被连接的主机。
如果是设计成两次握手，就有可能是被连接方第一次发出ack消息后，就处于成功建立连接的状态，但这条消息丢失了，主动连接方因为没有收到这个ack消息会认为建立连接失败，也许会放弃连接或启动新的连接，但被连接方会一直监听那个它误认为成功的连接。
采用三次握手，前两次握手任何一次失败都会导致连接双方都处于未连接状态，第三次失败只会导致连接方处于成功状态，但做主动连接方，肯定会在连接不久后通过这个连接发送数据，这样就可以利用这个机制做进一步的容错。

Q:半连接队列
A:
服务器第一次收到客户端的 SYN 之后，就会处于 SYN_RCVD 状态，此时双方还没有完全建立其连接，服务器会把此种状态下请求连接放在一个队列里，我们把这种队列称之为半连接队列。
当然还有一个全连接队列，就是已经完成三次握手，建立起连接的就会放在全连接队列中。如果队列满了就有可能会出现丢包现象。
这里在补充一点关于SYN-ACK 重传次数的问题：
服务器发送完SYN-ACK包，如果未收到客户确认包，服务器进行首次重传，等待一段时间仍未收到客户确认包，进行第二次重传。如果重传次数超过系统规定的最大重传次数，系统将该连接信息从半连接队列中删除。

Q: https传输过程
A:
1，https是基于tcp协议的，客户端先会和服务端发起链接建立
2，接着服务端会把它的证书返回给客户端，证书里面包括公钥S.pub、颁发机构和有效期等信息
3，拿到的证书可以通过浏览器内置的根证书（内含C.pub）验证其合法性
4，客户端生成随机的对称加密秘钥Z，通过服务端的公钥S.pub加密发给服务端
5，客户端和服务端通过对称秘钥Z加密数据来进行http通信

Q:ca认证过程
A:
1-服务器会预先生成非对称加密密钥，私钥S.pri自己保留；而公钥S.pub则发给CA机构进行签名认证
2-CA也会预先生成一非对称加密密钥，其私钥C.pri用来对服务器的公钥S.pub进行签名生成CA证书
3-CA机构会把签名生成的CA证书返回给服务器，也就是刚才服务端给客户端那个证书
4-因为CA(证书颁发机构)比较权威，所以很多浏览器会内置包含它公钥(C.pub)的证书，称之为根证书。然后可以使用根证书来验证其颁发证书的合法性了
![](res/2021-03-27-15-21-03.png)

证书通常包含这些内容(1) 服务端的公钥；(2) 证书发行者(CA)对证书的数字签名；(3) 证书所用的签名算法；(4) 证书发布机构、有效期、所有者的信息等其他信息

Q: MD5、SHA、Base64和RSA
A: 
MD5、SHA，称为摘要算法，可以归类为单向加密算法，其计算出的摘要信息，是不可逆向恢复成原来的数据
RSA属于非对称加密算法
Base64并不算是加密算法，它更多时候是被称为一种数据编码方式
加密算法分三大类：单向加密，对称加密算法和非对称加密算法

Q:ISN(Initial Sequence Number)是固定的吗
A:
当一端为建立连接而发送它的SYN时，它为连接选择一个初始序号。ISN随时间而变化，因此每个连接都将具有不同的ISN。ISN可以看作是一个32比特的计数器，每4ms加1 。这样选择序号的目的在于防止在网络中被延迟的分组在以后又被传送，而导致某个连接的一方对它做错误的解释。
三次握手的其中一个重要功能是客户端和服务端交换 ISN(Initial Sequence Number)，以便让对方知道接下来接收数据的时候如何按序列号组装数据。如果 ISN 是固定的，攻击者很容易猜出后续的确认号，因此 ISN 是动态生成的。

Q: 三次握手过程中可以携带数据吗？
A:其实第三次握手的时候，是可以携带数据的。但是，第一次、第二次握手不可以携带数据
为什么这样呢?大家可以想一个问题，假如第一次握手可以携带数据的话，如果有人要恶意攻击服务器，那他每次都在第一次握手中的 SYN 报文中放入大量的数据。因为攻击者根本就不理服务器的接收、发送能力是否正常，然后疯狂着重复发 SYN 报文的话，这会让服务器花费很多时间、内存空间来接收这些报文。
也就是说，第一次握手不可以放数据，其中一个简单的原因就是会让服务器更加容易受到攻击了。而对于第三次的话，此时客户端已经处于 ESTABLISHED 状态。对于客户端来说，他已经建立起连接了，并且也已经知道服务器的接收、发送能力是正常的了，所以能携带数据也没啥毛病。

Q:半连接队列
A:
SYN攻击就是Client在短时间内伪造大量不存在的IP地址，并向Server不断地发送SYN包，Server则回复确认包，并等待Client确认，由于源地址不存在，因此Server需要不断重发直至超时，这些伪造的SYN包将长时间占用未连接队列，导致正常的SYN请求因为队列满而被丢弃，从而引起网络拥塞甚至系统瘫痪。SYN 攻击是一种典型的 DoS/DDoS 攻击。
常见的防御 SYN 攻击的方法有如下几种：
缩短超时（SYN Timeout）时间
增加最大半连接数
过滤网关防护
SYN cookies技术

Q: 四次挥手
A:
刚开始双方都处于 ESTABLISHED 状态，假如是客户端先发起关闭请求。四次挥手的过程如下：
第一次挥手：客户端发送一个 FIN 报文，报文中会指定一个序列号。此时客户端处于 FIN_WAIT1 状态。
即发出连接释放报文段（FIN=1，序号seq=u），并停止再发送数据，主动关闭TCP连接，进入FIN_WAIT1（终止等待1）状态，等待服务端的确认。
第二次挥手：服务端收到 FIN 之后，会发送 ACK 报文，且把客户端的序列号值 +1 作为 ACK 报文的序列号值，表明已经收到客户端的报文了，此时服务端处于 CLOSE_WAIT 状态。
即服务端收到连接释放报文段后即发出确认报文段（ACK=1，确认号ack=u+1，序号seq=v），服务端进入CLOSE_WAIT（关闭等待）状态，此时的TCP处于半关闭状态，客户端到服务端的连接释放。客户端收到服务端的确认后，进入FIN_WAIT2（终止等待2）状态，等待服务端发出的连接释放报文段。
第三次挥手：如果服务端也想断开连接了，和客户端的第一次挥手一样，发给 FIN 报文，且指定一个序列号。此时服务端处于 LAST_ACK 的状态。
即服务端没有要向客户端发出的数据，服务端发出连接释放报文段（FIN=1，ACK=1，序号seq=w，确认号ack=u+1），服务端进入LAST_ACK（最后确认）状态，等待客户端的确认。
第四次挥手：客户端收到 FIN 之后，一样发送一个 ACK 报文作为应答，且把服务端的序列号值 +1 作为自己 ACK 报文的序列号值，此时客户端处于 TIME_WAIT 状态。需要过一阵子以确保服务端收到自己的 ACK 报文之后才会进入 CLOSED 状态，服务端收到 ACK 报文之后，就处于关闭连接了，处于 CLOSED 状态。
即客户端收到服务端的连接释放报文段后，对此发出确认报文段（ACK=1，seq=u+1，ack=w+1），客户端进入TIME_WAIT（时间等待）状态。此时TCP未释放掉，需要经过时间等待计时器设置的时间2MSL后，客户端才进入CLOSED状态。
收到一个FIN只意味着在这一方向上没有数据流动。客户端执行主动关闭并进入TIME_WAIT是正常的，服务端通常执行被动关闭，不会进入TIME_WAIT状态。
在socket编程中，任何一方执行close()操作即可产生挥手操作。

为什么需要四次
因为当服务端收到客户端的SYN连接请求报文后，可以直接发送SYN+ACK报文。其中ACK报文是用来应答的，SYN报文是用来同步的。但是关闭连接时，当服务端收到FIN报文时，很可能并不会立即关闭SOCKET，所以只能先回复一个ACK报文，告诉客户端，“你发的FIN报文我收到了”。只有等到我服务端所有的报文都发送完了，我才能发送FIN报文，因此不能一起发送。故需要四次挥手。

Q: 2MSL
A:
TIME_WAIT状态也成为2MSL等待状态。每个具体TCP实现必须选择一个报文段最大生存时间MSL（Maximum Segment Lifetime），它是任何报文段被丢弃前在网络内的最长时间。这个时间是有限的，因为TCP报文段以IP数据报在网络内传输，而IP数据报则有限制其生存时间的TTL字段。
对一个具体实现所给定的MSL值，处理的原则是：当TCP执行一个主动关闭，并发回最后一个ACK，该连接必须在TIME_WAIT状态停留的时间为2倍的MSL。这样可让TCP再次发送最后的ACK以防这个ACK丢失（另一端超时并重发最后的FIN）。
这种2MSL等待的另一个结果是这个TCP连接在2MSL等待期间，定义这个连接的插口（客户的IP地址和端口号，服务器的IP地址和端口号）不能再被使用。这个连接只能在2MSL结束后才能再被使用。

等待2MSL的意义
MSL是Maximum Segment Lifetime的英文缩写，可译为“最长报文段寿命”，它是任何报文在网络上存在的最长时间，超过这个时间报文将被丢弃。
为了保证客户端发送的最后一个ACK报文段能够到达服务器。因为这个ACK有可能丢失，从而导致处在LAST-ACK状态的服务器收不到对FIN-ACK的确认报文。服务器会超时重传这个FIN-ACK，接着客户端再重传一次确认，重新启动时间等待计时器。最后客户端和服务器都能正常的关闭。假设客户端不等待2MSL，而是在发送完ACK之后直接释放关闭，一但这个ACK丢失的话，服务器就无法正常的进入关闭连接状态。
两个理由：
保证客户端发送的最后一个ACK报文段能够到达服务端。
这个ACK报文段有可能丢失，使得处于LAST-ACK状态的B收不到对已发送的FIN+ACK报文段的确认，服务端超时重传FIN+ACK报文段，而客户端能在2MSL时间内收到这个重传的FIN+ACK报文段，接着客户端重传一次确认，重新启动2MSL计时器，最后客户端和服务端都进入到CLOSED状态，若客户端在TIME-WAIT状态不等待一段时间，而是发送完ACK报文段后立即释放连接，则无法收到服务端重传的FIN+ACK报文段，所以不会再发送一次确认报文段，则服务端无法正常进入到CLOSED状态。
防止“已失效的连接请求报文段”出现在本连接中。
客户端在发送完最后一个ACK报文段后，再经过2MSL，就可以使本连接持续的时间内所产生的所有报文段都从网络中消失，使下一个新的连接中不会出现这种旧的连接请求报文段。

为什么TIME_WAIT状态需要经过2MSL才能返回到CLOSE状态？
理论上，四个报文都发送完毕，就可以直接进入CLOSE状态了，但是可能网络是不可靠的，有可能最后一个ACK丢失。所以TIME_WAIT状态就是用来重发可能丢失的ACK报文。

Q: TCP和UDP有什么区别
A: 
TCP协议是有连接的，有连接的意思是开始传输实际数据之前TCP的客户端和服务器端必须通过三次握手建立连接，会话结束之后也要结束连接。而UDP是无连接的
TCP协议保证数据按序发送，按序到达，提供超时重传来保证可靠性，但是UDP不保证按序到达，甚至不保证到达，只是努力交付，即便是按序发送的序列，也不保证按序送到。
TCP协议所需资源多，TCP首部需20个字节（不算可选项），UDP首部字段只需8个字节。
TCP有流量控制和拥塞控制，UDP没有，网络拥堵不会影响发送端的发送速率
TCP是一对一的连接，而UDP则可以支持一对一，多对多，一对多的通信。
UDP是面向报文传输的， 应用层交给UDP多长的报文，UDP就照样发送，即一次发送一个报文 ；TCP是面向字节流传输的， TCP把应用程序发来的数据看成是一连串的字节流 ，等缓冲区缓冲了一定的字节就发送一次
TCP： 效率要求低，对准确性要求高的场景 。 eg：文件传输（准确高要求高、但是速度可以相对慢）、接受邮件、远程登录
UDP： 效率要求高，准确性要求低的场景 。eg: 视频聊天、语音电话（即时通讯，速度要求高，但是出现偶尔断续不是太大问题，并且此处完全不可以使用重发机制）、广播


Q: 在浏览器中输入 网址 后执行的全部过程
A: 
1. 输入网址
2. DNS查找
    浏览器缓存：不同浏览器缓存时间不同
    系统缓存
    搜索操作系统的hosts文件
    路由器缓存
    ISP DNS 缓存
    本地域名服务器 向根域名服务器发起请求，根域名服务器返回com域的顶级域名服务器的地址；本地域名服务器 向com域的顶级域名服务器发起请求，返回baidu.com权限域名服务器（权限域名服务器，用来保存该区中的所有主机域名到IP地址的映射）地址；本地域名服务器 向baidu.com权限域名服务器发起请求，得到www.baidu.com的IP地址
    本地域名服务器 将得到的IP地址返回给操作系统，同时自己也将IP地址缓存起来
3. 建立 TCP 连接，三次握手
4. 浏览器给web服务器发送一个HTTP请求
5. 服务器给浏览器响应一个301永久重定向响应
6. 浏览器知道了正确地址，会发送另一个获取请求
7. 服务器发回一个HTML响应
8. 浏览器开始显示HTML
9. 浏览器发送获取嵌入在HTML中的对象(js css)
10. 浏览器发送异步（AJAX）请求

Q: 流量控制和拥塞控制
A: 
流量控制：如果发送方把数据发送得过快，接收方可能会来不及接收，这就会造成数据的丢失。
拥塞控制：拥塞控制就是防止过多的数据注入到网络中，这样可以使网络中的路由器或链路不致过载。
流量控制指点对点通信量的控制。而拥塞控制是全局性的，涉及到所有的主机和降低网络性能的因素

流量控制有两种方式
停止-等待流量控制基本原理：发送方每发送一帧，都要等待接收方的应答信号，之后才能发送下一帧；接收方每接受一帧，都要反馈一个应答信号，表示可接受下一帧，如果接收方不反馈应答信号，则发送方必须一直等待。每次只允许发送一帧，然后就陷入等待接收方确认信息的过程中，因而传输效率很低。
滑动窗口流量控制基本原理：
在任意时刻，发送方都维持一组连续的允许发送的帧的序号，称为发送窗口；同时接收方也维持一组连续的允许接收帧的序号，称为接收窗口。发送窗口的大小代表在还没有收到对方确认信息的情况下发送方最多还可以发送多少个数据帧。同理，在接收端设置接收窗口是为了控制可以接收哪些数据帧而不可以接收哪些数据帧。在发送端，每收到一个确认帧，发送窗口就向前滑动一个帧的位置，当发送窗口内没有可以发送的帧（即窗口内的帧全部是已发送但未收到确认的帧），发送方就会停止发送，直到收到接受方发送的确认帧使窗口移动，窗口内有可以发送的帧，之后才开始继续发送。
在接受端，当收到数据帧后，将窗口向前移一个位置，并发回确认帧，若收到的数据帧落在接受窗口之外则一律丢弃。

从滑动窗口的概念看，停止-等待协议、后退N帧协议和选择重传协议只有在发送窗口大小和接受窗口大小有所差别。
停止-等待协议：发送窗口大小=1，接受窗口大小=1；
后退N帧协议：发送窗口大小>1，接受窗口大小=1；
选择重传协议：发送窗口大小>1，接受窗口大小>1；

数据链路层的可靠传输通常使用确认和超时重传两种机制来完成。
确认是一种无数据的控制帧，这种控制帧使得接收方可以让发送方知道哪些内容被正确接收。有些情况下为了提高传输效率，将确认捎带在一个回复帧中，称为捎带确认。
超时重传是指发送方在发送某一个数据帧以后就开始一个计时器，在一定时间内如果没有得到发送的数据帧的确认帧，那么就重新发送该数据帧，直到发送成功为止。
自动重传请求（Auto Repeat reQuest，ARQ），通过接收方请求发送方重传出错的数据帧来恢复出错的帧

拥塞控制
发送方控制拥塞窗口的原则是：只要网络没有出现拥塞，拥塞窗口就再增大一些，以便把更多的分组发送出去。但只要网络出现拥塞，拥塞窗口就减小一些，以减少注入到网络中的分组数。

发送端判断拥塞发生的依据：
1.传输超时（TCP重传定时器溢出）
2.接收到重复的确认报文段
拥塞控制对这两种情况有不同的处理方式。对第一种情况仍然使用慢启动和拥塞避免。对第二种情况使用快速重传和快速恢复（如果是真的发生拥塞的话），注意：第二种情况如果发生在第一种情况之后，则也被拥塞控制当成第一种情况来对待

慢开始算法：
拥塞窗口和接收窗口共同决定了发送者的发送窗口
当主机开始发送数据时，如果立即所大量数据字节注入到网络，那么就有可能引起网络拥塞，因为现在并不清楚网络的负荷情况。
较好的方法是 先探测一下，即由小到大逐渐增大发送窗口，也就是说，由小到大逐渐增大拥塞窗口数值
通常在刚刚开始发送报文段时，先把拥塞窗口 cwnd 设置为一个最大报文段MSS的数值。而在每收到一个对新的报文段的确认后，把拥塞窗口增加至多一个MSS的数值。用这样的方法逐步增大发送方的拥塞窗口 cwnd ，可以使分组注入到网络的速率更加合理。

拥塞避免算法：
让拥塞窗口cwnd缓慢地增大，即每经过一个往返时间RTT就把发送方的cwnd 拥塞窗口cwnd加1cwnd ，而不是加倍cwnd 。这样拥塞窗口cwnd按线性规律缓慢增长，比慢开始算法的拥塞窗口增长速率缓慢得多。
不论是慢开始还是拥塞避免只要网络出现拥塞（没有按时到达）时，就把ssthresh的值置为出现拥塞时的拥塞窗口的一半（但不能小于2），以及cwnd置为1，进行慢开始。目的是迅速减少主机发送到网络中的分组数，使得发生拥塞的路由器有足够时间把队列中积压的分组处理完毕。

拥塞控制算法需要判断当收到重复的确认报文端时，网络是否真的发生了阻塞，或者说TCP报文端是否真的丢失了。具体的做法是：发送端如果连续收到3个重复的确认报文端，就认为是拥塞发生了。然后它启用快速重传和快速恢复算法来处理拥塞。

快速重传(Fast retransmit):要求接收方在收到一个失序的报文段后就立即发出重复确认（为的是使发送方及早知道有报文段没有到达对方），而不要等到自己发送数据时捎带确认。即使是收到了失序的报文段也要立即发出对已收到的报文段的重复确认。
如果在超时重传定时器溢出之前，接收到连续的三个重复冗余ACK（其实是收到4个同样的ACK，第一个是正常的，后三个才是冗余的），发送端便知晓哪个报文段在传输过程中丢失了，于是重发该报文段，不需要等待超时重传定时器溢出，大大提高了效率。这便是快速重传机制。
发送方收到三个重复确认，就知道只是丢失了个别的报文段，于是不启动慢开始算法而是快恢复算法，
将慢开始门限和拥塞窗口都调整为当前窗口的一半，开始执行拥塞避免算法。

当出现超时重传和冗余ack的时候慢启动门限都要设置为当前发送窗口的一半
不同的就是超时重传还得将拥塞窗口大小设为1，重新进入慢启动，而冗余ack则是将拥塞窗口设为慢启动门限大小并且进入拥塞避免


Q: 有哪些socket
A: 

Q: TCP的粘包和拆包问题
A:
程序需要发送的数据大小和TCP报文段能发送MSS（Maximum Segment Size，最大报文长度）是不一样的
大于MSS时，而需要把程序数据拆分为多个TCP报文段，称之为拆包；小于时，则会考虑合并多个程序数据为一个TCP报文段，则是粘包；其中MSS = TCP报文段长度-TCP首部长度
在IP协议层或者链路层、物理层，都存在拆包、粘包现象

解决粘包和拆包的方法
在数据尾部增加特殊字符进行分割
将数据定为固定大小
将数据分为两部分，一部分是头部，一部分是内容体；其中头部结构大小固定，且有一个字段声明内容体的大小

Q: 什么是TCP粘包？怎么解决这个问题
A: 
TCP粘包就是指发送方发送的若干包数据到达接收方时粘成了一包，从接收缓冲区来看，后一包数据的头紧接着前一包数据的尾，出现粘包的原因是多方面的，可能是来自发送方，也可能是来自接收方。
发送方原因：TCP默认使用Nagle算法（主要作用：减少网络中报文段的数量）。而Nagle算法主要做两件事：只有上一个分组得到确认，才会发送下一个分组；收集多个小分组，在一个确认到来时一起发送；Nagle算法造成了发送方可能会出现粘包问题
接收方原因：TCP接收到数据包时，并不会马上交到应用层进行处理，或者说应用层并不会立即处理。实际上，TCP将接收到的数据包保存在接收缓存里，然后应用程序主动从缓存读取收到的分组。这样一来，如果TCP接收数据包到缓存的速度大于应用程序从缓存中读取数据包的速度，多个包就会被缓存，应用程序就有可能读取到多个首尾相接粘到一起的包。

什么时候需要处理粘包现象？
如果发送方发送的多组数据本来就是同一块数据的不同部分，比如说一个文件被分成多个部分发送，这时当然不需要处理粘包现象；如果多个分组毫不相干，甚至是并列关系，那么这个时候就一定要处理粘包现象了

怎么解决
发送方：对于发送方造成的粘包问题，可以通过关闭Nagle算法来解决，使用TCP_NODELAY选项来关闭算法
接收方：接收方没有办法来处理粘包现象，只能将问题交给应用层来处理。
应用层：循环处理，应用程序从接收缓存中读取分组时，读完一条数据，就应该循环读取下一条数据，直到所有数据都被处理完成，但是如何判断每条数据的长度呢？
格式化数据：每条数据有固定的格式（开始符，结束符），这种方法简单易行，但是选择开始符和结束符时一定要确保每条数据的内部不包含开始符和结束符。
发送长度：发送每条数据时，将数据的长度一并发送，例如规定数据的前4位是数据的长度，应用层在处理时可以根据长度来判断每个分组的开始和结束位置。

UDP 会不会产生粘包问题
TCP为了保证可靠传输并减少额外的开销（每次发包都要验证），采用了基于流的传输，基于流的传输不认为消息是一条一条的，是无保护消息边界的（保护消息边界：指传输协议把数据当做一条独立的消息在网上传输，接收端一次只能接受一条独立的消息）。
UDP则是面向消息传输的，是有保护消息边界的，接收方一次只接受一条独立的信息，所以不存在粘包问题。

## 操作系统

Q: cache 和 buffer

Q: 段页式
A:
对用户而言，分段是对内存的有效使用；而对于计算机而言，分页可以提高内存的使用效率。操作系统需要满足两个方面的需求，所以就采取了段页相结合的方式来管理内存。
对于用户而言，当用户发出一个逻辑地址，用户希望访问到特定程序段的内存空间，而对于计算机而言，则希望用户发出的逻辑可以通过MMU转换成页框号和页内偏移量，从而直接去访问真实的内存空间。
为了解决这一问题引入了虚拟内存（就是通过一张段表完成地址映射转换）：简单的说就是用户发出访问程序段的逻辑地址<段号，段内偏移量>，通过对这一逻辑地址的运算将其转换为访问页的虚拟地址<页号，页内偏移量>，再由MMU将其转换为内存的物理地址<页框号，页内偏移量>。通过这种方式，用户访问的就是虚拟内存，经过两次地址映射后，变成真实的物理地址。

页式和段式管理策略都不会产生外部碎片，但都有可能产生内部碎片
页的大小是统一的，而段的大小是可变的
采用分页会导致用户视角的内存和实际内存的分离，即使用户视角的内存和实际物理内存不一样，而分段正好可以支持用户视角，使用户视角的内存和实际物理内存分布保持一致
分页对程序员来说是透明的，用户指定一个地址，该地址通过硬件分为页码和偏移，这些程序员是看不见的；而分段对程序员来说通常是可见的，用户通过两个量：段号和偏移来指定地址，这两个量作为组织程序和数据的一种方便手段提供给程序员，程序员可以通过这两个量把程序和数据指定到不同的段（程序员必须清楚段的最大长度）

Q:局部性原理
A:
时间局部性(Temporal locality):
如果某个信息这次被访问，那它有可能在不久的未来被多次访问。时间局部性是空间局部性访问地址一样时的一种特殊情况。这种情况下，可以把常用的数据加cache来优化访存。

空间局部性(Spatial locality):
如果某个位置的信息被访问，那和它相邻的信息也很有可能被访问到。这个也很好理解，我们大部分情况下代码都是顺序执行，数据也是顺序访问的。

内存局部性(Memory locality):
访问内存时，大概率会访问连续的块，而不是单一的内存地址，其实就是空间局部性在内存上的体现。目前计算机设计中，都是以块/页为单位管理调度存储，其实就是在利用空间局部性来优化性能。

分支局部性(Branch locality)
这个又被称为顺序局部性，计算机中大部分指令是顺序执行，顺序执行和非顺序执行的比例大致是5:1，即便有if这种选择分支，其实大多数情况下某个分支都是被大概率选中的，于是就有了CPU的分支预测优化。

等距局部性(Equidistant locality)
等距局部性是指如果某个位置被访问，那和它相邻等距离的连续地址极有可能会被访问到，它位于空间局部性和分支局部性之间。举个例子，比如多个相同格式的数据数组，你只取其中每个数据的一部分字段，那么他们可能在内存中地址距离是等距的，这个可以通过简单的线性预测就预测是未来访问的位置。

Q: 函数压栈过程
A:
函数调用大概包括以下几个步骤：
(1)参数入栈：将参数从右向左依次压入系统栈中。
(2)返回地址入栈：将当前代码区调用指令的下一条指令地址压入栈中，供函数返回时继续执行。
(3)代码区跳转：处理器从当前代码区跳转到被调用函数的入口处。
(4)栈帧调整：具体包括：
    <1>保存当前栈帧状态值，已备后面恢复本栈帧时使用(EBP入栈)。
    <2>将当前栈帧切换到新栈帧(将ESP值装入EBP，更新栈帧底部)。
    <3>给新栈帧分配空间(把ESP减去所需空间的大小，抬高栈顶)。

函数返回的步骤如下：
<1>保存返回值，通常将函数的返回值保存在寄存器EAX中。
<2>弹出当前帧，恢复上一个栈帧。具体包括：
    (1)在堆栈平衡的基础上，给ESP加上栈帧的大小，降低栈顶，回收当前栈帧的空间。
    (2)将当前栈帧底部保存的前栈帧EBP值弹入EBP寄存器，恢复出上一个栈帧。
    (3)将函数返回地址弹给EIP寄存器。
<3>跳转：按照函数返回地址跳回母函数中继续执行。


Q: screen
A:
screen创建了一个socket(一般是在/var/run/screen/下面)，把它伪装成为一个虚拟文本终端 （pty device）来充当标准输入输出设备，然后调用了一个shell 程序让它运行在这个文本终端（其实是个socket）上。当你从一个xterm里头detach掉了screen，screen下运行的程序不会直接退 出，因为它们用的标准输入输入装置已经不是你的xterm(下面跑的那个shell)所相关的那个虚拟文本终端，而是一个socket了。

Q: nohup 和 &
A:
都是转到后台执行，但是关闭终端还是会发出SIGHUP信号到这个进程下的子进程，导致进程关闭；而nohup会忽略SIGHUP命令。

Q: 共享内存
A:
共享内存，顾名思义就是允许两个不相关的进程访问同一个逻辑内存，共享内存是两个正在运行的进程之间共享和传递数据的一种非常有效的方式
共享内存并未提供同步机制，也就是说，在第一个进程结束对共享内存的写操作之前，并无自动机制可以阻止第二个进程开始对它进行读取，所以我们通常需要用其他的机制来同步对共享内存的访问，例如信号量。
在Linux中，每个进程都有属于自己的进程控制块（PCB）和地址空间（Addr Space），并且都有一个与之对应的页表，负责将进程的虚拟地址与物理地址进行映射，通过内存管理单元（MMU）进行管理。两个不同的虚拟地址通过页表映射到物理空间的同一区域，它们所指向的这块区域即共享内存

Q：fork函数
A:

![](res/2021-03-23-19-21-03.png)

Q: 多进程或多线程的区别
A:
<https://blog.csdn.net/linraise/article/details/12979473>

Q: 进程控制块（PCB）
A:
进程描述信息：
process ID、进程名、user ID、process group
进程控制信息
当前状态、优先级、代码执行入口地址、程序的外存地址、进程间同步和通信
资源占用信息
虚拟地址空间的现状、打开文件列表
CPU现场保护结构
寄存器值、PC、栈指针地址等。

组织方式：同一状态的进程使用一个链表。如就绪链表、阻塞链表

Q: 用户态切换到内核态的3种方式
A：
一般现代CPU都有几种不同的指令执行级别。在高执行级别下，代码可以执行特权指令，访问任意的物理地址，这种CPU执行级别就对应着内核态。而在相应的低级别执行状态下，代码的掌控范围会受到限制。只能在对应级别允许的范围内活动。intel x86 CPU有四种不同的执行级别0-3，linux只使用了其中的0级和3级分别来表示内核态和用户态。

当我们在系统中执行一个程序时，大部分时间是运行在用户态下的，在其需要操作系统帮助完成某些它没有权力和能力完成的工作时就会切换到内核态

系统调用，用户态进程主动要求切换到内核态的一种方式
当CPU在执行运行在用户态下的程序时，发生了某些事先不可知的异常，这时会触发由当前运行进程切换到处理此异常的内核相关程序中，也就转到了内核态，比如缺页异常。
外围设备的中断：当外围设备完成用户请求的操作后，会向CPU发出相应的中断信号，这时CPU会暂停执行下一条即将要执行的指令转而去执行与中断信号对应的处理程序，如果先前执行的指令是用户态下的程序，那么这个转换的过程自然也就发生了由用户态到内核态的切换。比如硬盘读写操作完成，系统会切换到硬盘读写的中断处理程序中执行后续操作等。

Q: 输出 HelloWorld 的过程
A:
1.用户告诉操作系统执行hello程序
2.操作系统找到该程序，检查其类型
3.检查程序首部，找出正文和数据的地址
4.文件系统找到第一个磁盘块
5.父进程需要创建一个新的子进程，执行hello程序
6.操作系统需要将执行文件映射到进程结构
7.操作系统设置CPU上下文环境，并跳到程序开始处
8.程序的第一条指令执行，失败，缺页中断发生
9.操作系统分配一页内存，并将代码从磁盘读入，继续执行
10.更多的缺页中断，读入更多的页面
11.程序执行系统调用，在文件描述符中写一字符串
12.操作系统检查字符串的位置是否正确
13.操作系统找到字符串被送往的设备
14.设备是一个伪终端，由一个进程控制
15.操作系统将字符串送给该进程
16.该进程告诉窗口系统它要显示字符串
17.窗口系统确定这是一个合法的操作，然后将字符串转换成像素
18.窗口系统将像素写入存储映像区
19.视频硬件将像素表示转换成一组模拟信号控制显示器（重画屏幕）
20.显示器发射电子束
21.你在屏幕上看到hello world


Q: inode 硬链接 软连接
A:
inode 数据文件索引信息
1.存储一个数据属性信息（类型 权限 链接数 属主/属组 大小 时间）
2.存储指向相应的block指针信息
1. 数据存储会占用一个inode

硬链接文件指向相同inode
源文件删除不会影响硬链接文件
只能给普通文件创建硬链接，不能给目录创建硬链接

软链接文件会指向到源文件
源文件删除链接文件失效
可以给目录创建软链接

Q: 缺页中断
A: 
当一个进程发生缺页中断的时候，进程会陷入内核态，执行以下操作：
1.检查要访问的虚拟地址是否合法
2.查找/分配一个物理页
3.填充物理页内容（读取磁盘，或者直接置0，或者啥也不干）
4.建立映射关系（虚拟地址到物理地址）
5.重新执行发生缺页中断的那条指令

Q: 内存分配（malloc）的过程
A: 
进程分配内存有两种方式，分别由两个系统调用完成：brk和mmap。brk是将数据段(.data)的最高地址指针_edata往高地址推；mmap是在进程的虚拟地址空间中（堆和栈中间，称为文件映射区域的地方）找一块空闲的虚拟内存。
malloc小于128k的内存，使用brk分配内存，将_edata往高地址推。最高地址空间的空闲内存超过128K（可由M_TRIM_THRESHOLD选项调节）时，执行内存紧缩操作
malloc大于128k的内存，使用mmap分配内存，在堆和栈之间找一块空闲内存分配(对应独立内存，而且初始化为0)。

Q: 从源代码到可执行程序的步骤
A:
1.预处理：修改源码，展开宏和#include头文件，最后生成一个.i文件
2.编译：检查代码规范性，语法错误等，生成一个汇编语言.s文件
3.汇编：.s文件翻译成二进制机器指令.o文件
4.链接：链接所有的函数、全局变量

Q: 操作系统读写文件的过程

读文件
1.进程调用库函数向内核发起读文件请求；
2.内核通过检查进程的文件描述符定位到虚拟文件系统的已打开文件列表表项；
3.调用该文件可用的系统调用函数read()
4.read()函数通过文件表项链接到目录项模块，根据传入的文件路径，在目录项模块中检索，找到该文件的inode；
5.在inode中，通过文件内容偏移量计算出要读取的页；
6.通过inode找到文件对应的address_space；
7.在address_space中访问该文件的页缓存树，查找对应的页缓存结点：
  (1) 如果页缓存命中，那么直接返回文件内容；
  (2) 如果页缓存缺失，那么产生一个页缺失异常，创建一个页缓存页，同时通过inode找到文件该页的磁盘地址，读取相应的页填充该缓存页；重新进行第6步查找页缓存；
8.文件内容读取成功。

写文件
前5步和读文件一致，在address_space中查询对应页的页缓存是否存在：
1.如果页缓存命中，直接把文件内容修改更新在页缓存的页中。写文件就结束了。这时候文件修改位于页缓存，并没有写回到磁盘文件中去。
2.如果页缓存缺失，那么产生一个页缺失异常，创建一个页缓存页，同时通过inode找到文件该页的磁盘地址，读取相应的页填充该缓存页。此时缓存页命中，进行第6步。
3.一个页缓存中的页如果被修改，那么会被标记成脏页。脏页需要写回到磁盘中的文件块。有两种方式可以把脏页写回磁盘：
  (1) 手动调用sync()或者fsync()系统调用把脏页写回
  (2) pdflush进程会定时把脏页写回到磁盘
同时注意，脏页不能被置换出内存，如果脏页正在被写回，那么会被设置写回标记，这时候该页就被上锁，其他写请求被阻塞直到锁释放。

Q: 编译器重排
A:
目前CPU执行程序指令都是流水线模式，执行操作涉及多个步骤，比如CPU获取、解码、运算操作，一个指令都需要涉及到多个步骤，如果能将类似指令进行重排序后能优化流水线的使用（前提是重排序后不会对执行结果造成影响），那么就可以提高性能

Q: 互斥量和信号量的区别
A: 
互斥量用于线程的互斥，信号量用于线程的同步
互斥锁底层的实现其实是基于计算机系统中原子操作（即在同一个时间段内，只有一个CPU能够执行某个操作，如访问某个内存位置）来完成的。
互斥：是指某一资源同时只允许一个访问者对其进行访问，具有唯一性和排它性。但互斥无法限制访问者对资源的访问顺序，即访问是无序的。
同步：是指在互斥的基础上（大多数情况），通过其它机制实现访问者对资源的有序访问。在大多数情况下，同步已经实现了互斥，特别是所有写入资源的情况必定是互斥的。少数情况是指可以允许多个访问者同时访问资源

互斥量值只能为0/1，信号量值可以为非负整数
也就是说，一个互斥量只能用于一个资源的互斥访问，它不能实现多个资源的多线程互斥问题。信号量可以实现多个同类资源的多线程互斥和同步。当信号量为单值信号量是，也可以完成一个资源的互斥访问。

互斥量的加锁和解锁必须由同一线程分别对应使用，信号量可以由一个线程释放，另一个线程得到。

Q: 虚拟内存
A:
![](res/2021-03-29-16-51-07.png)
![](res/2021-03-29-16-51-28.png)
![](res/2021-03-29-16-51-43.png)

Q: 进程、线程和协程之间的区别和联系
A: 

![](res/2021-03-29-16-48-54.png)
![](res/2021-03-29-16-49-39.png)
![](res/2021-03-29-16-50-18.png)
![](res/2021-03-29-16-55-12.png)

进程，直观点说，保存在硬盘上的程序运行以后，会在内存空间里形成一个独立的内存体，这个内存体有自己独立的地址空间，有自己的堆，上级挂靠单位是操作系统。操作系统会以进程为单位，分配系统资源（CPU时间片、内存等资源），进程是资源分配的最小单位。
线程，有时被称为轻量级进程(Lightweight Process，LWP），是操作系统调度（CPU调度）执行的最小单位。

Q: 死锁
A:
![](res/2021-03-29-16-56-20.png)

Q: 孤儿进程和僵尸进程
A: 

孤儿进程：一个父进程退出，而它的一个或多个子进程还在运行，那么那些子进程将成为孤儿进程。孤儿进程将被init进程（进程号为1）所收养，并由init进程对他们完成状态收集工作。
僵尸进程：当进程exit()退出之后，他的父进程没有通过wait()系统调用回收他的进程描述符的信息，该进程会继续停留在系统的进程表中，占用内核资源，这样的进程就是僵尸进程。

Q: 什么是IO多路复用
A: IO多路复用是一种同步IO模型，实现一个线程可以监视多个文件句柄；一旦某个文件句柄就绪，就能够通知应用程序进行相应的读写操作；没有文件句柄就绪时会阻塞应用程序，交出cpu。多路是指网络连接，复用指的是同一个线程
没有IO多路复用机制时，有同步阻塞、同步非阻塞两种实现方式。同步阻塞里，如果服务端采用单线程，当accept一个请求后，在recv或send调用阻塞时，将无法accept其他请求；如果采用多线程，当accept一个请求后，开启线程进行recv，可以完成并发处理，但随着请求数增加需要增加系统线程，大量的线程占用很大的内存空间，并且线程切换会带来很大的开销。同步非阻塞里，服务器端当accept一个请求后，加入fds集合，每次轮询一遍fds集合recv(非阻塞)数据，没有数据则立即返回错误，每次轮询所有fd（包括没有发生读写事件的fd）会很浪费cpu

select缺点
单个进程所打开的FD是有限制的，通过FD_SETSIZE设置，默认1024
每次调用select，都需要把fd集合从用户态拷贝到内核态，这个开销在fd很多时会很大
对socket扫描时是线性扫描，采用轮询的方法，效率较低（高并发时）

poll缺点
每次调用poll，都需要把fd集合从用户态拷贝到内核态，这个开销在fd很多时会很大
对socket扫描时是线性扫描，采用轮询的方法，效率较低（高并发时）

epoll有EPOLLLT和EPOLLET两种触发模式，LT是默认的模式，ET是“高速”模式。
LT模式下，只要这个fd还有数据可读，每次 epoll_wait都会返回它的事件，提醒用户程序去操作
ET模式下，它只会提示一次，直到下次再有数据流入之前都不会再提示了，无论fd中是否还有数据可读。所以在ET模式下，read一个fd的时候一定要把它的buffer读完，或者遇到EAGAIN错误

![select poll epoll](res/2021-03-17-21-04-17.png)


## 数据库

Q: Mysql和Mongodb
A:
MySQL是关系型数据库，Mongodb是非关系型数据库(nosql ),属于文档型数据库。
将mongodb作为 类似redis、memcache来做缓存db，为mysql提供服务，或是后端日志收集分析。
MongoDB的应用场景：表结构不明确且数据不断变大，内容管理或者博客平台等；更高的写入负载；数据量很大或者将来会变得很大，MongoDB内建了sharding、很多数据分片的特性，容易水平扩展，比较好的适应大数据量增长的需求；自带高可用，自动主从切换（副本集
不适用的场景
1）MongoDB不支持事务操作，需要用到事务的应用建议不用MongoDB。
2）MongoDB目前不支持join操作，需要复杂查询的应用也不建议使用MongoDB。

Q: 主从复制
A:
mysql主从复制用途
- 实时灾备，用于故障切换
- 读写分离，提供查询服务
- 备份，避免影响业务

主从部署必要条件：
- 主库开启binlog日志（设置log-bin参数）
- 主从server-id不同
- 从库服务器能连通主库

Q: MySQL主从同步中 主服务器宕机了如何处理，从服务器宕机如何处理
A:
主库宕机：
(1)确保所有的relay log全部更新完毕，在每个从库上执行show processlist
(2)更新完毕后，登录所有从库查看master.info文件，对比选择pos最大的作为新的主库，
(3)然后登录这个新的主库，执行stop slave；进入主目录，删除master.Info和relay-log.info配置my.cnf文件开启log-bin文件
(4)创建用于同步的用户并授权slave
(5)登录另外一台从库，执行stop slave停止同步
(6)执行start slave
(7)修改新的master数据，测试slave是否同步更新

从库宕机：
(1)查看从库上mysql的错误日志，里面有记录主从挂掉时的binlog信息。
(2)有了binlog和postion信息后，只需要重新在从库上进行change master to配置即可。配置后开启slave状态，没有报错
(3)查看slave状态，发现slave已经正常了，开始进行延时数据恢复。

Q: sql优化
A:
1.对查询进行优化，应尽量避免全表扫描，首先应考虑在 where 及 order by 涉及的列上建立索引
2.尽量避免在 where 子句中对字段进行 null 值判断，否则将导致引擎放弃使用索引而进行全表扫描
3.应尽量避免在 where 子句中使用!=或<>操作符，否则将引擎放弃使用索引而进行全表扫描
4.应尽量避免在 where 子句中使用 or 来连接条件，否则将导致引擎放弃使用索引而进行全表扫描
5.`in` 和 `not in` 也要慎用，否则会导致全表扫描
6.select id from t where name like '%abc%'
7.应尽量避免在 where 子句中对字段进行表达式操作，这将导致引擎放弃使用索引而进行全表扫描。
`select id from t where num/2=100` 应改为: `select id from t where num=100*2`
8.尽量避免在where子句中对字段进行函数操作，这将导致引擎放弃使用索引而进行全表扫描。
9.不要在 where 子句中的“=”左边进行函数、算术运算或其他表达式运算，否则系统将可能无法正确使用索引。    
10.在使用索引字段作为条件时，如果该索引是复合索引，那么必须使用到该索引中的第一个字段作为条件时才能保证系统使用该索引，否则该索引将不会被使用，并且应尽可能的让字段顺序与索引顺序相一致。
11.用 exists 代替 in
12.尽量使用数字型字段，若只含数值信息的字段尽量不要设计为字符型，这会降低查询和连接的性能，并会增加存储开销。这是因为引擎在处理查询和连接时会逐个比较字符串中每一个字符，而对于数字型而言只需要比较一次就够了。    
13.尽可能的使用 varchar 代替 char ，因为首先变长字段存储空间小，可以节省存储空间，其次对于查询来说，在一个相对较小的字段内搜索效率显然要高些。
14.不要使用 `select * from t` ，用具体的字段列表代替“*”，不要返回用不到的任何字段。
15.避免频繁创建和删除临时表，以减少系统表资源的消耗
16.尽量避免使用游标，因为游标的效率较差，如果游标操作的数据超过1万行，那么就应该考虑改写
17.尽量避免大事务操作，提高系统并发能力。
18.尽量避免向客户端返回大数据量

Q:WAL技术
A:Write-Ahead Logging 先写日志，再写磁盘。



Q: left join、inner join 和 right join
A:
![](res/2021-03-27-19-31-03.png)

Q:MVCC(Mutil-Version Concurrency Control)
A:
![](res/2021-03-29-21-43-48.png)
多版本并发控制，在Mysql的InnoDB引擎中就是指在已提交读(READ COMMITTD)和可重复读(REPEATABLE READ)这两种隔离级别下的事务对于SELECT操作会访问版本链中的记录的过程。

在InnoDB引擎表中，它的聚簇索引记录中有两个必要的隐藏列：
trx_id：每次对某条聚簇索引记录进行修改的时候的事务id
roll_pointer：存了一个指针，它指向这条聚簇索引记录的上一个版本的位置，通过它来获得上一个版本的记录信息

Q: 分区分表
A: 
分区，是指根据一定规则，将数据库中的一张表分解成多个更小的，容易管理的部分。从逻辑上看，只有一张表，但是底层却是由多个物理分区组成。
分表：指的是通过一定规则，将一张表分解成多张不同的表。比如将用户订单记录根据时间成多个表。
分表与分区的区别在于：分区从逻辑上来讲只有一张表，而分表则是将一张表分解成多张表。

表分区有什么好处
分区表的数据可以分布在不同的物理设备上，从而高效地利用多个硬件设备。和单个磁盘或者文件系统相比，可以存储更多数据
优化查询。在where语句中包含分区条件时，可以只扫描一个或多个分区表来提高查询效率；涉及sum和count语句时，也可以在多个分区上并行处理，最后汇总结果。
分区表更容易维护。例如：想批量删除大量数据可以清除整个分区。
可以使用分区表来避免某些特殊的瓶颈，例如InnoDB的单个索引的互斥访问，ext3问价你系统的inode锁竞争等。

分区表的限制因素
一个表最多只能有1024个分区
如果分区字段中有主键或者唯一索引的列，那么多有主键列和唯一索引列都必须包含进来。即：分区字段要么不包含主键或者索引列，要么包含全部主键和索引列。
分区表中无法使用外键约束
MySQL的分区适用于一个表的所有数据和索引，不能只对表数据分区而不对索引分区，也不能只对索引分区而不对表分区，也不能只对表的一部分数据分区。

Q: 隔离级别
A:
Serializable (串行化)：可避免脏读、不可重复读、幻读的发生。
Repeatable read (可重复读)：可避免脏读、不可重复读的发生。
Read committed (读已提交)：可避免脏读的发生。
Read uncommitted (读未提交)：最低级别，任何情况都无法保证。

Q: 乐观锁和悲观锁
A:
悲观锁（Pessimistic Lock）, 就是很悲观，每次去拿数据的时候都认为别人会修改。所以每次在拿数据的时候都会上锁。这样别人想拿数据就被挡住，直到悲观锁被释放。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。
乐观锁（Optimistic Lock）,总是假设最好的情况，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号机制和CAS算法实现。乐观锁适用于多读的应用类型，这样可以提高吞吐量，像数据库提供的类似于write_condition机制，其实都是提供的乐观锁。
悲观锁阻塞事务，乐观锁回滚重试。乐观锁适用于写比较少的情况下（多读场景），一般多写的场景下用悲观锁就比较合适。

版本号机制
一般是在数据表中加上一个数据版本号version字段，表示数据被修改的次数，当数据被修改时，version值会加一。当线程A要更新数据值时，在读取数据的同时也会读取version值，在提交更新时，若刚才读取到的version值为当前数据库中的version值相等时才更新，否则重试更新操作，直到更新成功。

CAS算法 compare and swap（比较与交换），是一种有名的无锁算法
CAS算法涉及到三个操作数
需要读写的内存值 V
进行比较的值 A
拟写入的新值 B
当且仅当 V 的值等于 A时，CAS通过原子方式用新值B来更新V的值，否则不会执行任何操作（比较和替换是一个原子操作）。一般情况下是一个自旋操作，即不断的重试。
```C++
data = 123; // 共享数据

/* 更新数据的线程会进行如下操作 */
flag = true;
while (flag) {
    oldValue = data; // 保存原始数据
    newValue = doSomething(oldValue); 

    // 下面的部分为CAS操作，尝试更新data的值
    if (data == oldValue) { // 比较
        data = newValue; // 设置
        flag = false; // 结束
    } else {
	// 啥也不干，循环重试
    }
}
/* 
   很明显，这样的代码根本不是原子性的，
   因为真正的CAS利用了CPU指令，
   这里只是为了展示执行流程，本意是一样的。
*/
```

乐观锁的缺点
1.ABA 问题:如果一个变量V初次读取的时候是A值，并且在准备赋值的时候检查到它仍然是A值,在这段时间它的值可能被改为其他值，然后又改回A，那CAS操作就会误认为它从来没有被修改过。这个问题被称为CAS操作的 "ABA"问题。
2.循环时间长开销大:自旋CAS（也就是不成功就一直循环执行直到成功）如果长时间不成功，会给CPU带来非常大的执行开销


Q: 为什么使用 B+ 树，不使用 B 树 红黑树 AVL树
A: 
B树：非叶子结点也可能存储数据 索引范围小 不支持范围查询 每个节点都有data域（指针），这无疑增大了节点大小，增加了磁盘IO次数
红黑树 AVL树：二叉树 树高较大 索引次数多 增多了IO开销

Q: 自增主键相对非自增主键的好处
A: 自增主键每次插入新的记录，记录就会顺序添加到当前索引节点的后续位置，当一页写满，就会自动开辟一个新的页；使用非自增主键，每次插入主键的值近似于随机，因此每次新纪录都要被插到现有索引页得中间某个位置，此时MySQL不得不为了将新记录插到合适位置而移动数据，甚至目标页面可能已经被回写到磁盘上而从缓存中清掉，此时又要从磁盘上读回来，这增加了很多开销，同时频繁的移动、分页操作造成了大量的碎片，得到了不够紧凑的索引结构，后续不得不通过OPTIMIZE TABLE来重建表并优化填充页面。

Q: memcached与redis的区别
A: 
1.存储方式不同。memcached把数据全部存在内存之中，断电之后会挂掉，而redis虽然也用到了内存，但是会有部分数据存在硬盘中，保证数据持久性。
2.数据支持类型不同。memcached对数据支持比较简单，而redis支持数据类型较丰富，如string、list、set、sorted set、hash。
3.底层实现不同。一般调用系统函数，会消耗比较多的时间去请求，redis自己构建了vm，速度会更快。
缓存类型：Redis和Memcache都是将数据存放在内存中，都是内存数据库。不过Memcache还可用于缓存其他东西，例如图片、视频等等。
存储数据结构：Redis不仅仅支持简单的k/v类型的数据，同时还提供list，set，hash，sorted set等数据结构的存储。
虚拟内存：Redis当物理内存用完时，可以将一些很久没用到的value 交换到磁盘
过期策略：Memcache在set时就指定，例如set key1 0 0 8,即永不过期。Redis可以通过例如expire 设定，例如expire name 10
分布式：设定Memcache集群，利用magent做一主多从;redis可以做一主多从。都可以一主一从
存储数据安全：Memcache挂掉后，数据没了；redis可以定期保存到磁盘（持久化）
灾难恢复：Memcache挂掉后，数据不可恢复; redis数据丢失后可以通过aof恢复
数据备份：Redis支持数据的备份，即master-slave模式的数据备份。

Q: 为什么 Redis 需要把所有数据放到内存中？
A: Redis为了达到最快的读写速度将数据都读到内存中，并通过异步的方式将数据写入磁盘。所以redis具有快速和数据持久化的特征。如果不将数据放在内存中，磁盘I/O速度会严重影响redis的性能。
如果设置了最大使用的内存，则数据已有记录数达到内存限值后不能继续插入新值，会执行内存淘汰机制。

Q: 分布式锁
A:
![分布式锁](res/2021-03-17-16-43-31.png)

Q: redis的并发竞争问题如何解决
A: 
redis为单进程单线程模式，采用队列模式将并发访问变为串行访问。redis本身时没有锁的概念的，redis对多个客户端连接并不存在竞争，但是在Jedis客户端对redis进行并发访问时会产生一系列问题，这些问题时由于客户端连接混乱造成的。有两种方案解决。
  1.在客户端，对连接进行池化，同时对客户端读写redis操作采用内部锁synchronized。
  2.在服务器角度，利用setnx实现锁。

## Redis

Q: Redis 阻塞原因
A:
内在原因
    1.不合理地使用API或数据结构
    慢查询(对一个包含上万个元素的hash结构执行hgetall操作，算法复杂度是O(n))
    解决：
    (1) 慢查询统计功能，执行 `slowlog get{n}` 命令可以获取最近的n条慢查询命令，默认对于执行超过10毫秒的命令都会记录到一个定长队列中，线上实例建议设置为1毫秒便于及时发现毫秒级以上的命令。
    (2) 调整修改为低算法复杂度的命令：调整大对象：缩减大对象数据或把大对象拆分为多个小对象，防止一次命令操作过多的数据。大对象拆分过程需要视具体的业务决定，如用户好友集合存储在Redis中，有些热点用户会关注大量好友，这时可以按时间或其他维度拆分到多个集合中。
    (3) 如何发现大对象：`redis-cli -h {ip} -p {port} --bigkeys`
    2.CPU饱和
    单线程的Redis处理命令时只能使用一个CPU。而CPU饱和是指Redis把单核CPU使用率跑到接近100%。使用top命令很容易识别出对应Redis进程的CPU使用率。CPU饱和是非常危险的，将导致Redis无法处理更多的命令，严重影响吞吐量和应用方的稳定性。
    解决：使用统计命令 `redis-cli -h {ip} -p {port} --stat` 获取当前Redis使用情况
    3.持久化阻塞
    主要针对开启了持久化功能的Redis节点。
    fork阻塞：fork操作发生在RDB和AOF重写时，Redis主线程调用fork操作产生共享内存的子进程，由子进程完成持久化文件重写工作。如果fork操作本身耗时过长，必然会导致主线程的阻塞。
    AOF刷盘阻塞：当我们开启AOF持久化功能时，文件刷盘的方式一般采用每秒一次，后台线程每秒对AOF文件做fsync操作。当硬盘压力过大时，fsync操作需要等待，直到写入完成。如果主线程发现距离上一次的fsync成功超过2秒，为了数据安全性它会阻塞直到后台线程执行fsync操作完成。这种阻塞行为主要是硬盘压力引起。
    HugePage写操作阻塞：子进程在执行重写期间利用Linux写时复制技术降低内存开销，因此只有写操作时Redis才复制要修改的内存页。对于开启Transparent HugePages的操作系统，每次写命令引起的复制内存页单位由4K变为2MB，放大了512倍，会拖慢写操作的执行时间，导致大量写操作慢查询。
外在原因
    1.CPU竞争
    进程竞争：Redis是典型的CPU密集型应用，不建议和其他多核CPU密集型服务部署在一起。当其他进程过度消耗CPU时，将严重影响Redis吞吐量。可以通过top、sar等命令定位到CPU消耗的时间点和具体进程，这个问题比较容易发现，需要调整服务之间部署结构。
    绑定CPU：部署Redis时为了充分利用多核CPU，通常一台机器部署多个实例。常见的一种优化是把Redis进程绑定到CPU上，用于降低CPU频繁上下文切换的开销。这个优化技巧正常情况下没有问题，但是存在例外情况，当Redis父进程创建子进程进行RDB/AOF重写时，如果做了CPU绑定，会与父进程共享使用一个CPU。子进程重写时对单核CPU使用率通常在90%以上，父进程与子进程将产生激烈CPU竞争，极大影响Redis稳定性。因此对于开启了持久化或参与复制的主节点不建议绑定CPU。
    2.内存交换
    内存交换(swap)对于Redis来说是非常致命的，Redis保证高性能的一个重要前提是所有的数据在内存中。如果操作系统把Redis使用的部分内存换出到硬盘，由于内存与硬盘读写速度差几个数量级，会导致发生交换后的Redis性能急剧下降。
    解决
    (1) 预防内存交换：保证机器充足的可用内存。
    (2) 确保所有Redis实例设置最大可用内存(maxmemory)，防止极端情况下Redis内存不可控的增长。
    (3) 降低系统使用swap优先级。
    3.网络问题
    (1) 连接拒绝网络闪断(网络割接或者带宽耗尽); Redis连接拒绝(超过客户端最大连接数); 连接溢出(进程限制或backlog队列溢出)
    (2) 网络延迟
    常见的物理拓扑按网络延迟由快到慢可分为：同物理机>同机架>跨机架>同机房>同城机房>异地机房。但它们容灾性正好相反，同物理机容灾性最低而异地机房容灾性最高。
    网络延迟问题经常出现在跨机房的部署结构上，对于机房之间延迟比较严重的场景需要调整拓扑结构，如把客户端和Redis部署在同机房或同城机房等。
    (3) 网卡软中断
    网卡软中断是指由于单个网卡队列只能使用一个CPU，高并发下网卡数据交互都集中在同一个CPU，导致无法充分利用多核CPU的情况。网卡软中断瓶颈一般出现在网络高流量吞吐的场景。

## 分布式

Q: Dockerfile的指令？
A: 
FROM：指定基础镜像；
MAINTAINER: 指定维护者信息
LABEL：功能是为镜像指定标签；
RUN：运行指定的命令；
CMD：容器启动时要运行的命令。
ADD：copy文件，会自动解压
WORKDIR：设置工作目录
VOLUME：设置卷，挂载主机目录

Q: 联合文件系统 (UnionFS)
A:
docker的镜像实际上由一层一层的文件系统组成，这种层级的文件系统就是UnionFS。UnionFS是一种分层、轻量级并且高性能的文件系统。联合加载会把各层文件系统叠加起来，这样最终的文件系统会包含所有底层的文件和目录。
![](res/2021-04-01-16-50-30.png)

Q: docker和虚拟机
A：
传统虚拟机技术是虚拟出一套硬件后，在其上运行一个完整操作系统，在该系统上再运行所需应用进程。容器内的应用进程直接运行于宿主的内核，容器内没有自己的内核，而且也没有进行硬件虚拟，因此容器要比传统虚拟机更为轻便。每个容器有自己的文件系统 ，容器之间进程不会相互影响，能区分计算资源。

Q: docker容器之间怎么隔离
A: 

Q: 分布式设计模式
A:
![](res/2021-04-01-16-11-41.png)
单容器管理者模式 (Single-container management patterns)：为容器增加一些可控接口，比如 run(), stop(), pause()，使得容器对外来说是可控的。
单节点-多容器应用模式 (Single-node, multi-container application patterns)。在K8s体系中，Pod是一个轻量级的节点，同一个Pod中的容器可以共享同一块存储空间和同一个网络地址空间，这使得我们可以实现一些组合多个容器在同一节点工作的模式。
    副载模式（Sidecar pattern）
        这种模式主要是利用在同一Pod中的容器可以共享存储空间的能力。
        应用场景1：一个工具容器写文件到共享的文件目录，应用主容器从共享的文件目录读文件。工具容器的镜像可以重用，而不需要跟应用的容器打包在一起。
        应用场景2：一个工具容器读文件，应用容器写文件。工具容器的镜像是可以重用的，不需要在每次更新应用容器打包的时候，把工具容器的执行文件打包进去。
    大使模式（Ambassador pattern）
        这种模式主要利用同一Pod中的容器可以共享网络地址空间的特性。
        在一个Pod中给应用容器搭配一个工具容器作为代理服务器。工具容器帮助应用容器访问外部服务，使得应用容器访问服务时不需要使用外网的IP地址，而只需要用localhost访问本地服务。在这种模式下，作为代理服务器的工具容器好像外部服务派驻在Pod中的“外交官”，使得应用容器办理业务时只需要跟本Pod的外交官打交道，而不需要出国了，因此而得名。
    适配器模式（Adapter pattern）
        这种模式对于监控和管理分布式系统尤为重要。
        应用和监控管理交互的接口需要是统一的，而且其接口是依照“统一的监控服务”的接口模式来实现。
多节点应用模式 (Multi-node application patterns)
    领导人选举 (Leader election pattern)
    工作队列模式 (Work queue pattern)
        把需要处理的任务放到一个待处理的队列里，根据需要启动计算节点从队列读取任务进行处理
    向量化模式 (Scatter/gather pattern)
        分散收集模式利用分布式系统弹性计算能力的容器设计模式
        服务的客户端将初始计算请求发送给一个“根计算节点”。根计算节点对计算任务做出分割，将任务分割成大量的小计算任务，然后将小计算任务分配给大量计算服务器进行分布式平行计算 。每个计算服务器都计算初始计算任务的一小块，将计算结果返回给根计算节点。根计算节点将所有计算结果合并起来，组成一个针对初始计算任务的一个统一的结果，返回给申请计算任务的客户端。

Q: 心跳检测的作用：
A:
检测主服务器的网络连接状态；辅助实现min-slaves选项；检测命令丢失。

Q: 消息队列
A:
场景：异步、削峰、解耦
异步：将流程拆分，只需要把完成的消息塞入消息队列告诉其他子系统
削峰：秒杀活动中把请求放入消息队列

Q: nginx 负载均衡
A: 负载均衡就是把很多请求进行分流，将他们分配到不同的服务器去处理。比如我有3个服务器，分别为A、B、C，然后使用Nginx进行负载均衡，使用轮询策略，此时如果收到了9个请求，那么会均匀的将这9个请求分发给A、B、Cf服务器，每一个服务器处理3个请求，这样的话我们可以利用多台机器集群的特性减少单个服务器的压力。（重定向）

Round Robin: 对所有的请求进行轮询发送请求，默认的分配方式。
Least Connections：以最少的活动连接数将请求发送到服务器，同样要考虑服务器权重。指定轮询几率，weight和访问比率成正比
IP Hash : 发送请求的服务器由客户机IP地址决定。在这种情况下，使用IPv4地址的前三个字节或整个IPv6地址来计算散列值。该方法保证来自相同地址的请求到达相同的服务器，除非该服务器不可用。
Generic Hash: 请求发送到的服务器由用户定义的键决定，该键可以是文本字符串、变量或组合

Q: 分布式事务的理解
A:
事务的参与者、支持事务的服务器、资源服务器以及事务管理器分别位于不同的分布式系统的不同节点之上。简单点理解就是： 一个在不同环境（比如不同的数据库、不同的机器上）下运行的业务，并且这些操作又需要在同一个事务中完成。

Q: 接口的幂等性
A: 论是微服务中各个子系统相互之间的调用，还是客户端对服务端的调用，都存在网络延迟等问题，会导致重复请求接口，这时候接口就需要支持幂等性，来防止出现问题。
逻辑判断处理：支付时对订单状态进行判断，如果该订单已支付，则不应该再次进行扣款操作。
请求带ticket：异步请求获取ticket，此ticket是唯一并且一次性的，保存在页面中，每次发起支付请求都带上ticket，后端检查ticket，若支付成功则删除ticket，这样就算重复提交也不会导致重复扣款。

Q: 分布式中的CAP
A: 
![CAP](res/2021-03-18-21-18-32.png)

Q: 单线程的设计为什么能支持高并发
A:
请求 Redis 更多的是操作内存。直接操作内存就很快
单线程，没有 CPU 上下文切换带来的开销问题。不用考虑各种锁
多路 IO 复用
由于是单线程的，所以就存在一个顺序读写问题
Redis 的数据结构
Redis 自己构建了VM 机制 。因为一般的调用系统函数，会浪费一定的时间。

Q: 一致性哈希算法
A: 在分布式系统中，为了保证负载均衡，可以对发送的信息进行hash映射，然而，如果节点增加或减少，将会使得系统所有的数据节点映射改变，给系统带来不稳定性。一致性哈希对2的32次方取模，节点增加或减少对系统的影响很小。若增加或减少节点，造成不命中，只需在哈希环上顺时针找到最近的一个节点即可。
由于哈希环节点可能稠密不同，为了保证负载均衡，可以对称增加虚拟节点。

Q:什么时候使用redis
A:
不需要实时更新但是又极其消耗数据库的数据。比如网站上商品销售排行榜，这种数据一天统计一次就可以了，用户不会关注其是否是实时的。
需要实时更新，但是更新频率不高的数据。比如一个用户的订单列表，他肯定希望能够实时看到自己下的订单，但是大部分用户不会频繁下单。
在某个时刻访问量极大而且更新也很频繁的数据。这种数据有一个很典型的例子就是秒杀，在秒杀那一刻，可能有N倍于平时的流量进来，系统压力会很大。但是这种数据使用的缓存不能和普通缓存一样，这种缓存必须保证不丢失，否则会有大问题。

Q: 大型网站系统架构
A: 
用户-负载均衡器-N台服务器-redis缓存集群-mysql集群
前端限流（例如一个用户10秒内只能点击一次，异步处理，消息队列）；
负载均衡一般采用NGINX反向代理；
mysql读写分离，主库写，从库读，分库分表。


Q: 两阶段提交协议（2PC）
A: 

两阶段提交协议（2PC）用来保证跨节点操作的原子性。该协议中，将系统节点分为：协调者和事务参与者。
正常执行过程如下：
1. 请求阶段(表决)：协调者通知事务参与者准备提交（事务参与者本地执行成功）或者取消（事务参与者本地执行失败）事务。
2. 提交阶段(执行)：协调者根据请求阶段的结果进行决策：提交或者取消。只有所有事务参与者都同意提交时，协调者才会通知所有的参与者提交事务，否则取消事务。

两阶段提交协议可能面临的故障：
1. 同步阻塞问题。执行过程中，所有参与节点都是事务阻塞型的。当参与者占有公共资源时，其他第三方节点访问公共资源不得不处于阻塞状态。
2. 协调者发生故障：由于协调者的重要性，一旦协调者发生故障。
参与者会一直阻塞下去。尤其在第二阶段，协调者发生故障，那么所有的参与者还都处于锁定事务资源的状态中，而无法继续完成事务操作。（如果是协调者挂掉，可以重新选举一个协调者，但是无法解决因为协调者宕机导致的参与者处于阻塞状态的问题）
3. 数据不一致。在二阶段提交的阶段二中，当协调者向参与者发送commit请求之后，发生了局部网络异常或者在发送commit请求过程中协调者发生了故障，这回导致只有一部分参与者接受到了commit请求。而在这部分参与者接到commit请求之后就会执行commit操作。但是其他部分未接到commit请求的机器则无法执行事务提交。于是整个分布式系统便出现了数据不一致性的现象。

两阶段提交无法解决的问题
当协调者出错，同时参与者也出错时，两阶段无法保证事务执行的完整性。
考虑协调者在发出commit消息之后宕机，而唯一接收到这条消息的参与者同时也宕机了。
那么即使协调者通过选举协议产生了新的协调者，这条事务的状态也是不确定的，没人知道事务是否被已经提交。

Q: 三阶段提交 (3PC)，还有其和2PC的区别
A: 

三阶段的执行
三阶段提交协议在协调者和参与者中都引入超时机制，并且把两阶段提交协议的第一个阶段分成了两步: 询问，然后再锁资源，最后真正提交。

1. canCommit阶段: 3PC的canCommit阶段其实和2PC的准备阶段很像。协调者向参与者发送commit请求，参与者如果可以提交就返回yes响应，否则返回no响应
2. preCommit阶段：协调者根据参与者canCommit阶段的响应来决定是否可以继续事务的preCommit操作。根据响应情况，有下面两种可能: (a) 协调者从所有参与者得到的反馈都是yes: 那么进行事务的预执行，协调者向所有参与者发送preCommit请求，并进入prepared阶段。参与者和接收到preCommit请求后会执行事务操作，并将undo和redo信息记录到事务日志中。如果一个参与者成功地执行了事务操作，则返回ACK响应，同时开始等待最终指令；(b) 协调者从所有参与者得到的反馈有一个是No或是等待超时之后协调者都没收到响应: 那么就要中断事务，协调者向所有的参与者发送abort请求。参与者在收到来自协调者的abort请求，或超时后仍未收到协调者请求，执行事务中断。
3. doCommit阶段：协调者根据参与者preCommit阶段的响应来决定是否可以继续事务的doCommit操作。根据响应情况，有下面两种可能: (a) 协调者从参与者得到了ACK的反馈: 协调者接收到参与者发送的ACK响应，那么它将从预提交状态进入到提交状态，并向所有参与者发送doCommit请求。参与者接收到doCommit请求后，执行正式的事务提交，并在完成事务提交之后释放所有事务资源，并向协调者发送haveCommitted的ACK响应。那么协调者收到这个ACK响应之后，完成任务; （b）协调者从参与者没有得到ACK的反馈, 也可能是接收者发送的不是ACK响应，也可能是响应超时: 执行事务中断。

3PC vs 2PC
对于协调者(Coordinator)和参与者(Cohort)都设置了超时机制（在2PC中，只有协调者拥有超时机制，即如果在一定时间内没有收到cohort的消息则默认失败）。
在2PC的准备阶段和提交阶段之间，插入预提交阶段，使3PC拥有CanCommit、PreCommit、DoCommit三个阶段。
PreCommit是一个缓冲，保证了在最后提交阶段之前各参与节点的状态是一致的。
三阶段提交是“非阻塞”协议。
三阶段提交在两阶段提交的第一阶段与第二阶段之间插入了一个准备阶段，
使得原先在两阶段提交中，参与者在投票之后，由于协调者发生崩溃或错误，
而导致参与者处于无法知晓是否提交或者中止的“不确定状态”所产生的可能相当长的延时的问题得以解决。 举例来说，假设有一个决策小组由一个主持人负责与多位组员以电话联络方式协调是否通过一个提案，以两阶段提交来说，主持人收到一个提案请求，打电话跟每个组员询问是否通过并统计回复，然后将最后决定打电话通知各组员。
要是主持人在跟第一位组员通完电话后失忆，而第一位组员在得知结果并执行后老人痴呆，那么即使重新选出主持人，也没人知道最后的提案决定是什么，也许是通过，也许是驳回，不管大家选择哪一种决定，都有可能与第一位组员已执行过的真实决定不一致，老板就会不开心认为决策小组沟通有问题而解雇。
三阶段提交即是引入了另一个步骤，主持人打电话跟组员通知请准备通过提案，以避免没人知道真实决定而造成决定不一致的失业危机。
为什么能够解决二阶段提交的问题呢？
回到刚刚提到的状况，在主持人通知完第一位组员请准备通过后两人意外失忆，即使没人知道全体在第一阶段的决定为何，全体决策组员仍可以重新协调过程或直接否决，不会有不一致决定而失业。
那么当主持人通知完全体组员请准备通过并得到大家的再次确定后进入第三阶段，
当主持人通知第一位组员请通过提案后两人意外失忆，这时候其他组员再重新选出主持人后，
仍可以知道目前至少是处于准备通过提案阶段，表示第一阶段大家都已经决定要通过了，此时便可以直接通过。

三阶段提交协议的缺点
如果进入PreCommit后，Coordinator发出的是abort请求，假设只有一个Cohort收到并进行了abort操作，而其他对于系统状态未知的Cohort会根据3PC选择继续Commit，此时系统状态发生不一致性。

## 虚拟化技术

容器的四种网络模式: bridge 桥接模式、host 模式、container 模式和 none 模式

Q: k8s和docker有什么区别
A: Docker提供容器的生命周期管理，Docker镜像构建运行时容器。但是，由于这些单独的容器必须通信，因此使用Kubernetes。因此，我们说Docker构建容器，这些容器通过Kubernetes相互通信。因此，可以使用Kubernetes手动关联和编排在多个主机上运行的容器。

Q: 在主机和容器上部署应用程序有什么区别
A:
![](res/2021-03-31-15-27-00.png)
容器的中心思想就是秒级启动；一次封装、到处运行；这是主机部署应用无法达到的效果，但同时也更应该注重容器的数据持久化问题。另外，容器部署可以将各个服务进行隔离，互不影响，这也是容器的另一个核心概念。