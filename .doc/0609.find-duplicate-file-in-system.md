# [609.Find Duplicate File in System](https://leetcode.com/problems/find-duplicate-file-in-system/description/)

## Description

**Tags**: hash-table,string

**Companies**: Unknown

|  Category  |   Difficulty    | Likes | Dislikes |
| :--------: | :-------------: | :---: | :------: |
| algorithms | Medium (67.68%) | 1459  |   1625   |

<p>Given a list <code>paths</code> of directory info, including the directory path, and all the files with contents in this directory, return <em>all the duplicate files in the file system in terms of their paths</em>. You may return the answer in <strong>any order</strong>.</p>
<p>A group of duplicate files consists of at least two files that have the same content.</p>
<p>A single directory info string in the input list has the following format:</p>
<ul>
  <li><code>&quot;root/d1/d2/.../dm f1.txt(f1_content) f2.txt(f2_content) ... fn.txt(fn_content)&quot;</code></li>
</ul>
<p>It means there are <code>n</code> files <code>(f1.txt, f2.txt ... fn.txt)</code> with content <code>(f1_content, f2_content ... fn_content)</code> respectively in the directory &quot;<code>root/d1/d2/.../dm&quot;</code>. Note that <code>n &gt;= 1</code> and <code>m &gt;= 0</code>. If <code>m = 0</code>, it means the directory is just the root directory.</p>
<p>The output is a list of groups of duplicate file paths. For each group, it contains all the file paths of the files that have the same content. A file path is a string that has the following format:</p>
<ul>
  <li><code>&quot;directory_path/file_name.txt&quot;</code></li>
</ul>
<p>&nbsp;</p>
<p><strong class="example">Example 1:</strong></p>
<pre><code><strong>Input:</strong> paths = ["root/a 1.txt(abcd) 2.txt(efgh)","root/c 3.txt(abcd)","root/c/d 4.txt(efgh)","root 4.txt(efgh)"]
<strong>Output:</strong> [["root/a/2.txt","root/c/d/4.txt","root/4.txt"],["root/a/1.txt","root/c/3.txt"]]</code></pre><p><strong class="example">Example 2:</strong></p>
<pre><code><strong>Input:</strong> paths = ["root/a 1.txt(abcd) 2.txt(efgh)","root/c 3.txt(abcd)","root/c/d 4.txt(efgh)"]
<strong>Output:</strong> [["root/a/2.txt","root/c/d/4.txt"],["root/a/1.txt","root/c/3.txt"]]</code></pre>
<p>&nbsp;</p>
<p><strong>Constraints:</strong></p>
<ul>
  <li><code>1 &lt;= paths.length &lt;= 2 * 10<sup>4</sup></code></li>
  <li><code>1 &lt;= paths[i].length &lt;= 3000</code></li>
  <li><code>1 &lt;= sum(paths[i].length) &lt;= 5 * 10<sup>5</sup></code></li>
  <li><code>paths[i]</code> consist of English letters, digits, <code>&#39;/&#39;</code>, <code>&#39;.&#39;</code>, <code>&#39;(&#39;</code>, <code>&#39;)&#39;</code>, and <code>&#39; &#39;</code>.</li>
  <li>You may assume no files or directories share the same name in the same directory.</li>
  <li>You may assume each given directory info represents a unique directory. A single blank space separates the directory path and file info.</li>
</ul>
<p>&nbsp;</p>
<p><strong>Follow up:</strong></p>
<ul>
  <li>Imagine you are given a real file system, how will you search files? DFS or BFS?</li>
  <li>If the file content is very large (GB level), how will you modify your solution?</li>
  <li>If you can only read the file by 1kb each time, how will you modify your solution?</li>
  <li>What is the time complexity of your modified solution? What is the most time-consuming part and memory-consuming part of it? How to optimize?</li>
  <li>How to make sure the duplicated files you find are not false positive?</li>
</ul>

## Solution

**题目描述**

给定一个文件系统，其中包含许多文件，文件以路径和内容的形式给出。要求找到文件系统中的重复文件。重复文件指的是具有相同内容的文件，返回所有重复文件的路径集合。

**解题思路**

1. 哈希表
   - 遍历所有文件，将文件内容作为键，文件路径作为值，存入哈希表中。
   - 如果哈希表中已经存在该文件内容，则将该文件路径加入到结果集中。
   - 时间复杂度：假设文件系统中有 `N` 个文件，每个文件的平均大小为 `M`，构建哈希表的时间复杂度为 $O(NM)$，遍历哈希表的时间复杂度为 $O(N)$，因此总体时间复杂度为 $O(NM)$。
   - 空间复杂度：哈希表的空间复杂度为 $O(NM)$，存储结果的空间复杂度为 $O(N)$。
2. 哈希表+istringstream
   - 思路同上，不同的是使用 `istringstream` 来分割文件内容。
   - 时间复杂度：假设文件系统中有 `N` 个文件，每个文件的平均大小为 `M`，构建哈希表的时间复杂度为 $O(NM)$，遍历哈希表的时间复杂度为 $O(N)$，因此总体时间复杂度为 $O(NM)$。
   - 空间复杂度：哈希表的空间复杂度为 $O(NM)$，存储结果的空间复杂度为 $O(N)$。
3. 哈希表+MD5
   - 遍历所有文件，将文件内容的哈希值作为键，文件路径作为值，存入哈希表中。
   - 时间复杂度：假设文件系统中有 `N` 个文件，每个文件的平均大小为 `M`，构建哈希表的时间复杂度为 $O(NM)$，遍历哈希表的时间复杂度为 $O(N)$，因此总体时间复杂度为 $O(NM)$。
   - 空间复杂度：哈希表的空间复杂度为 $O(NM)$，存储结果的空间复杂度为 $O(N)$。

**标签**

- hash-table
- string

<!-- code start -->
## Code

### C++

```cpp
// 1. 哈希表
// 2023-06-20 submission
// 162/162 cases passed
// Runtime: 92 ms, faster than 72.47% of cpp online submissions.
// Memory Usage: 35.6 MB, less than 72.47% of cpp online submissions.
class Solution {
public:
    vector<vector<string>> findDuplicate(vector<string>& paths) {
        unordered_map<string, vector<string>> m;
        for (auto& path : paths) {
            stringstream ss(path);
            string root;
            string s;
            getline(ss, root, ' ');
            while (getline(ss, s, ' ')) {
                int idx = s.find('(');
                string filename = s.substr(0, idx);
                string content = s.substr(idx + 1, s.size() - idx - 2);
                m[content].push_back(root + '/' + filename);
            }
        }
        vector<vector<string>> res;
        for (auto& p : m) {
            if (p.second.size() > 1) {
                res.push_back(p.second);
            }
        }
        return res;
    }
};
```

```cpp
// 2. 哈希表+istringstream
// 2023-06-20 submission
// 162/162 cases passed
// Runtime: 120 ms, faster than 33.15% of cpp online submissions.
// Memory Usage: 44.6 MB, less than 29.77% of cpp online submissions.
class Solution {
public:
    vector<vector<string>> findDuplicate(vector<string>& paths) {
        vector<vector<string>> res;
        unordered_map<string, vector<string>> m;
        for (string path : paths) {
            istringstream is(path);
            string pre = "", t = "";
            is >> pre;
            while (is >> t) {
                int idx = t.find_last_of('(');
                string dir = pre + "/" + t.substr(0, idx);
                string content = t.substr(idx + 1, t.size() - idx - 2);
                m[content].push_back(dir);
            }
        }
        for (auto a : m) {
            if (a.second.size() > 1) res.push_back(a.second);
        }
        return res;
    }
};
```

```cpp
// 3. 哈希表 + MD5
// OJ 不支持 openssl
#include <openssl/md5.h>
class Solution {
public:
    string getFileHash(const string& filePath) {
        ifstream file(filePath, ios::binary);

        if (!file.is_open()) {
            return "";
        }

        MD5_CTX md5Context;
        MD5_Init(&md5Context);

        char buffer[4096];
        while (file.good()) {
            file.read(buffer, sizeof(buffer));
            MD5_Update(&md5Context, buffer, file.gcount());
        }

        unsigned char hash[MD5_DIGEST_LENGTH];
        MD5_Final(hash, &md5Context);

        stringstream ss;
        for (int i = 0; i < MD5_DIGEST_LENGTH; ++i) {
            ss << hex << static_cast<int>(hash[i]);
        }

        return ss.str();
    }

    vector<vector<string>> findDuplicate(vector<string>& paths) {
        unordered_map<string, vector<string>> hashMap;

        // traverse all paths
        for (const string& path : paths) {
            stringstream ss(path);
            string directory, file;
            string content;

            // get directory
            getline(ss, directory, ' ');

            // extract file name and content
            while (getline(ss, file, ' ')) {
                size_t leftParenthesis = file.find('(');
                size_t rightParenthesis = file.find(')');

                string fileName = file.substr(0, leftParenthesis);
                string fileContent =
                    file.substr(leftParenthesis + 1, rightParenthesis - leftParenthesis - 1);

                // get full path
                string fullPath = directory + '/' + fileName;

                // calculate file hash
                string fileHash = getFileHash(fullPath);

                // store file path
                hashMap[fileHash].push_back(fullPath);
            }
        }

        // 提取重复文件的路径集合
        vector<vector<string>> result;
        for (const auto& entry : hashMap) {
            if (entry.second.size() > 1) {
                result.push_back(entry.second);
            }
        }

        return result;
    }
};
```

<!-- code end -->
